---
title: "Report on Matching of Unpaywall and Web of Science"
authors:
  - "Nicholas Fraser"
  - "Anne Hobert"
output:
  html_document:
    keep_md: TRUE
    fig_caption: TRUE
---
## Initial setup

```{r setup, include=FALSE}

# knitr options
knitr::opts_chunk$set(echo = TRUE)

# Call libraries
library(tidyverse)
library(RJDBC)
library(DBI)
library(dbplyr)
library(viridis)

# Store also personal authentication credentials as kb_user01 and kb_password01 in .Renviron file.

# Open DB connection

drv <-JDBC("oracle.jdbc.OracleDriver", classPath= "jdbc_driver/ojdbc8.jar")
con <- dbConnect(drv, "jdbc:oracle:thin:@//biblio-p-db01:1521/bibliodb01.fiz.karlsruhe", Sys.getenv("kb_user01"), Sys.getenv("kb_password01"))

```

## Motivation

<!-- There is a growing need to monitor open access to scholarly literature. Information on the open access status of research publications is desired to study the development of open availability over time, phenomena connected with open access, like a possible citation advantage, to observe compliance with funder mandates and to inform decision making, be it on institutional level or in science policy.

Yet, gathering information on the open access status often is not a trivial task.-->

This report documents our approach to developing a procedure for the connection of items covered by the Web of Science (WoS) in-house database of the Competence Center for bibliometrics (KB) to information on their open access status contained in the Unpaywall data dump.

The most straight-forward way to perform a matching between WoS and Unpwaywall is based on [digital object identifiers (doi)](https://www.doi.org/), i.e. persistent interoperable identifiers. However, doi information in WoS is incomplete: not for all records a doi is stored in the database even though many of them are actually included in Unpaywall.

```{r}
wos_doi_cov <- dbGetQuery(con, read_file("sql/wos_doi_coverage.sql"))
write_csv(wos_doi_cov, "data/wos_doi_coverage.csv")
wos_doi_cov_tidy <- wos_doi_cov %>%
  gather("is_null", "number_of_items", -TOTAL_NUMBER_OF_ITEMS, -ARTICLE_TYPE, -PUBYEAR) %>%
  mutate(is_null = case_when(
    is_null == "NUMBER_OF_NULLS" ~ TRUE,
    is_null == "NUMBER_OF_DOIS" ~ FALSE
  )) %>%
  select(-TOTAL_NUMBER_OF_ITEMS)

# Total number of null DOIs per year
wos_doi_cov_tidy %>%
  group_by(PUBYEAR, is_null) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  ggplot(aes(x = PUBYEAR, y = number_of_items)) + 
    geom_bar(aes(fill = is_null), stat = "identity", position = "dodge") +
    labs(x = "Publication year", y = "Number of items", fill = "DOI is null?",
         title = "What number of items have null-DOI in WoS?")
```
Looking at all `r wos_doi_cov_tidy %>% summarise(n = sum(number_of_items)) %>% .$n` items in the WoS-KB bibliometric database (`wos_b_2019`) we see that only `r wos_doi_cov_tidy %>% filter(is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n` of them contain DOI information, corresponding to `r round((wos_doi_cov_tidy %>% filter(is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %.  The figure reveals that almost all of the items in WoS with a publication year before 2000 are missing any DOI information. Indeed, of `r wos_doi_cov_tidy %>% filter(PUBYEAR <2000) %>% summarise(n = sum(number_of_items)) %>% .$n` items older than 2000 in total, only `r wos_doi_cov_tidy %>% filter(PUBYEAR <2000, is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n` contain DOI information, corresponding to about `r round((wos_doi_cov_tidy %>% filter(PUBYEAR <2000, is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(PUBYEAR <2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %. In more recent years, the number of items including DOI information rises continuously. The coverage is much better here, with `r round((wos_doi_cov_tidy %>% filter(PUBYEAR >= 2000, is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(PUBYEAR >= 2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` % of items published from 2000 onwards having DOI information. The decline in the number of items with and without doi in the last years is probably due to an indexing lag.

```{r}
# Number of null DOIs within articles and reviews per year
wos_doi_cov_tidy %>%
  filter(ARTICLE_TYPE %in% c("article", "review")) %>%
  group_by(PUBYEAR, is_null) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  ggplot(aes(x = PUBYEAR, y = number_of_items)) + 
    geom_bar(aes(fill = is_null), stat = "identity", position = "dodge") +
    labs(x = "Publication year", y = "Number of articles", fill = "DOI is null?",
         title = "What number of articles and reviews have null-DOI in WoS?")
```

The observed trend is similar if we focus on the document types journal articles and reviews only. Overall, `r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review")) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` % of all articles and reviews contain DOI information. Again, the percentage is extremely low for publication years before 2000 (`r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE, PUBYEAR < 2000) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), PUBYEAR < 2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %), and higher for recent years (`r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE, PUBYEAR >= 2000) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), PUBYEAR >= 2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %). Moreover, for these document types, we observe that the number of items missing DOI information decreases continuously from 2000 onwards, leading to a coverage of `r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE, PUBYEAR == 2017) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), PUBYEAR == 2017) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` % in 2017.

We now want to investigate more thoroughly how the proportion of items without doi depends on the article type. In order to keep the figures readable, we first identify the most common article types and collate the remaining ones in the category `Other`. Looking at the number of items per article type in the following figures, we decide to keep all types with more than `100,000` items.

```{r}
wos_doi_cov_tidy %>%
  group_by(ARTICLE_TYPE) %>%
  summarise(n = sum(number_of_items)) %>%
  arrange(desc(n)) %>%
  mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE)) %>%
  ggplot(aes(ARTICLE_TYPE, n)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(x = "Article type", y = "Number of items", title = "Total number of items per article type")

wos_doi_cov_tidy %>%
  group_by(ARTICLE_TYPE) %>%
  summarise(n = sum(number_of_items)) %>%
  arrange(desc(n)) %>%
  mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE)) %>%
  ggplot(aes(ARTICLE_TYPE, n)) +
    geom_bar(stat = "identity") +
    scale_y_log10() +
    coord_flip() +
    labs(x = "Article type", y = "Number of items", title = "Total number of items per article type, logarithmic scale")
```

With this, we see the following behavior for the existence of a doi per article type.

```{r}
# Number of null DOIs per article type
# TODO: maybe make this a relative chart?
wos_doi_cov_tidy_factors <- wos_doi_cov_tidy %>%
  mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE))%>%
  group_by(ARTICLE_TYPE) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  arrange(desc(number_of_items)) %>%
  mutate(article_type_grouped = fct_reorder(fct_relevel(fct_other(ARTICLE_TYPE, keep = ARTICLE_TYPE[.$number_of_items > 0.01*sum(wos_doi_cov_tidy$number_of_items)]), "Other"), number_of_items))
wos_doi_cov_tidy %>%
  mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE))%>%
  mutate(article_type_grouped = fct_relevel(fct_other(ARTICLE_TYPE, keep = wos_doi_cov_tidy_factors$article_type_grouped), "Other")) %>%
  group_by(article_type_grouped, is_null) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  arrange(desc(number_of_items)) %>%
  ggplot(aes(x = fct_relevel(fct_reorder(article_type_grouped, desc(number_of_items)), "Other", after = Inf), y = number_of_items)) + 
    geom_bar(aes(fill = is_null), stat = "identity", position = "dodge") +
    coord_flip() + 
    labs(x = "Publication year", y = "Number of items", fill = "DOI is null?",
         title = "What article types have null-DOI in WoS?")
```
In this figure we see that missing DOIs are an issue particularly for proceedings papers, meeting abstracts, book reviews and marginal categories (`Other`). The proportion of journal articles and reviews without DOI is much lower, as the above results already indicated.

## Data selection and acquistion

To develop and test our matching strategy we decided to focus on data from Unpaywall with publication year 2014 to reduce storage space needed (the whole Unpaywall data dump needs more than 100 GB). For the most recent years often indexeing lags cause problems and, as just mentioned, for earlier years the DOI coverage within WoS is much worse.

We further decided to focus exlusively on journal articles and reviews since these are the article types covered best in WoS <!-- insert reference--> and we consider them to be most relevant for the studies and scenarios where our matching strategy might be applied.

This means that from the Unpaywall data we only consider records with publication year 2014 that have the `genre` `journal-article`. 

We obtained our set of Unpaywall articles similarly to how it is described in our [blog post on open access evidence in Unpaywall](https://subugoe.github.io/scholcomm_analytics/posts/unpaywall_evidence/). We will outline the main steps again in the following.

First, we downloaded the most recent Unpaywall data dump released in April 2019 from [here](https://s3-us-west-2.amazonaws.com/unpaywall-data-snapshots/) and imported it into a local MongoDB database. The snapshot contains more than 100 million records and hence, the file is about 100 GB large. To obtain a more manageable object to work with, we extracted the fields which are most relevant to our matching objective using the following query:

```{bash}
"C:\Program Files\MongoDB\Server\4.0\bin\mongo.exe"
db.unpaywallApr19.aggregate([
   {
      $match: {
          year: { $gte: 2014, $lte: 2016}
      }
   },
   {
      $project: {
          doi: 1,
          year: 1,
          genre: 1,
          title: 1,
          AuthorCount: { $cond: { if: { $isArray: "$z_authors" }, then: { $size: "$z_authors" }, else: null} },
          AuthorFirst: { $cond: { if: { $isArray: "$z_authors" }, then: { $arrayElemAt: [ "$z_authors", 0 ] }, else: null } },
          AuthorLast: { $cond: { if: { $isArray: "$z_authors" }, then: { $arrayElemAt: [ "$z_authors", -1 ] }, else: null } }
      }
   },
   {
      $out: 'authorInformation' 
   }
] )
```

Note that on a Linux or Mac OS, single quotes (`'`) and double quotes (`"`) need to be exchanged. We initially filtered for publication years between 2014 and 2016, extracted doi, publication year, article type (`genre`), article title, and some author information, namely the number of authors, and the given and family names of the first and last author. For single author articles, the first and last author coincide. We exported the resulting data as json file and loaded it as table into our [GoogleBigQuery](https://cloud.google.com/bigquery/) analytical environment (paid service, access protected), specifying a [schema](schema_bigquery.json). There, we added information on the journal (ISSN and journal title) to the data from a previous export based on the local MongoDB database. We joined the journal related fields on DOI, which was possible since every Unpaywall entry contains DOI information. We then exported the table as three different .csv files, one for each publication year and imported the one corresponding to the publication year 2014 into the oracle environment of the KB.

<!-- move this to criteria section

Since the registered publication dates in WoS and Unpaywall sometimes differ (for example, because one takes the online publication date and the other the print publication date), we compared publication years for the entries that could be matched based on DOI. The results are shown in the following figure.

```{r}

# Count total matches as a function of WOS publication year (Unpaywall
# publication year = 2014).

#Gather data
doi_matches_pubyears <- dbGetQuery(con, read_file("sql/doi_matches_publication_years.sql"))

#Display results
doi_matches_pubyears %>%
  ggplot(aes(x=WOS_YEAR, y=MATCHES)) +
  geom_bar(stat="identity") +
  labs(x="WOS Publication Year", y="Number of DOI matched items", title = "How do publication years differ between Unpaywall (2014) and WOS\nfor items matched based on DOIs?") +
  theme_bw()

```

Most items have the same publication year, 2014, in WoS as in the Unpaywall data dump. Some items are associated with one or two years later in WoS than in Unpaywall. A negligible amount of items (`round(doi_matches_pubyears %>% filter(WOS_YEAR %in% c(2012, 2017)) %>% summarise(n = sum(MATCHES))/sum(doi_matches_pubyears$MATCHES), 4)*100` percent of all matched items) is mapped to publication years 2012 or 2017 in WoS. Based on this, we decided to focus try to match items in Unpaywall with items from Wos having the same publication year, or one or two years later.

We also restrict the data from WoS to items with publication years between 2014 and 2016 and `article type` `article` or `review` in this analysis . -->


## Matching criteria

A number of features may be appropriate for matching articles between bibliographic databases <!-- reference?! -->. In the best case scenario, persistent identifiers such as DOIs can be used for direct matching. In the cases where persistent identifiers are missing, other characteristics of an article and its authorship may be used for matching, such as the article title, journal title or ISSN, or author names. However, none of these features are 100 % unique (e.g. articles can share titles, or authors can share names) and thus using them in isolation may lead to unreliable matches. A combination of these characteristics can produce more precise matching.

In this report we focus on an initial subset of characteristics which may potentially be used for matching between WOS and Unpaywall, including article titles, ISSNs, author counts. These criteria were selected primarily due to data availability and coverage in both databases, but are not exhaustive and may be supplemented with further characteristics in future (e.g. page numbers). 

Note that for the following sections, we refer to the cleaned and normalised datasets of WOS and Unpaywall documented in the previous section.

### Article titles

Article titles are potentially a good candidate for matching due to their high coverage: 

```{r}

# Should limit to Article and Review type documents?

wos_article_title_coverage <- dbGetQuery(con, read_file("sql/criteria/article_title_wos_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
upw_article_title_coverage <- dbGetQuery(con, read_file("sql/criteria/article_title_upw_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
wos_article_title_uniques <- dbGetQuery(con, read_file("sql/wos_article_title_uniques.sql")) %>% 
  pull(PCT_UNIQUE)
upw_article_title_uniques <- dbGetQuery(con, read_file("sql/upw_article_title_uniques.sql")) %>% 
  pull(PCT_UNIQUE)

```

`r wos_article_title_coverage` % of 'article' and 'review' type documents in WOS, and `r upw_article_title_coverage` % of 'journal-article' type documents in Unpaywall have a title. However, only `r wos_article_title_uniques` % of 'article' and 'review' titles in WOS are unique, compared to `r upw_article_title_uniques` of 'journal-articles' in Unpaywall, suggesting that direct matching of titles may lead to a high number of false-positive matches.

To better understand the the potential for using article titles for matching articles without DOIs, we can consider inverse set of DOI-matched articles (i.e. as a 'gold standard'), and compare their title characteristics. We first check the percentage of DOI-matched articles that also have exactly matching titles:

```{r}

# Calculate percentage of doi-matched articles with exactly matching titles

doi_matches_exact_titles <- dbGetQuery(con, read_file("sql/doi_matches_exact_titles.sql")) %>%
  pull(PCT_EXACT_TITLE_MATCHES)

```

We find that only `r doi_matches_exact_titles` % <!-- ~ 76% --> of doi-matched articles have exactly matching titles. <!-- What are reasons for low percentage of exact title matches? Character encoding / handling of special characters? -->

An alternative to exact matching is to consider approximate string matching. Oracle provides two built-in functions as part of the `UTL_MATCH` package for approximate string matching: `EDIT_DISTANCE_SIMILARITY()` AND `JARO_WINKLER_SIMILARITY()`. Both functions provide a similarity score that is normalised such that 1 equates to perfect similarity and 0 to no similarity (or 100 % and 0 % when given as percentages, respectively). `EDIT_DISTANCE_SIMILARITY()` provides a similarity score based upon the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two strings, which counts the number of edits (insertions, deletions, or substitutions) needed to convert one string to the other. `JARO_WINKLER_SIMILARITY()` provides a similarity score based on the [Jaro-Winkler distance](https://en.wikipedia.org/wiki/Jaro-Winkler_distance), which measures the number of characters in common and number of transpositions.

Below we plot a histogram of showing the proportion of matching titles in our DOI-matched set, as a function of the two similarity measures. Note that for performance reasons, we use a small sample of the Unpaywall results set (~5%, using the Oracle `SAMPLE(5)` function):

```{r}

#doi_matches_title_similarity <- dbGetQuery(con, read_file("sql/doi_matches_title_similarity.sql"))

doi_matches_title_similarity %>%
  select(EDIT_DISTANCE_SIMILARITY, JARO_WINKLER_SIMILARITY) %>%
  gather("EDIT_DISTANCE_SIMILARITY", "JARO_WINKLER_SIMILARITY", 
         key="method", 
         value="similarity") %>%
  group_by(method, similarity) %>%
  summarise(n = n()) %>%
  mutate(freq = n/sum(n)) %>%
  ggplot(aes(x=similarity, y=freq, fill=method, color=method)) +
  geom_bar(stat="identity") +
  labs(y="Percentage Matched", x="Title Similarity") +
  theme_bw() +
  theme(legend.position = "none") +
  scale_y_continuous(breaks=seq(from=0, to=1, by=0.1), labels=scales::percent) +
  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +
  facet_wrap(~method)

```

These plots show that `EDIT_DISTANCE_SIMILARITY()` and `JARO_WINKLER_SIMILARITY()` have similar distributions of similarity scores for titles on DOI-matched articles. A threshold appears to exist at approximately 90 % similarity<!-- or 80%? It depends how much you zoom in on the y-axis... -->, below which very few titles are matched, potentially indicating a suitable threshold below which title similarities should be considered unreliable.

An additional method to assess the efficacy of the different similarity matching methods is to assess matching similarity on randomly matched articles, i.e. assuming that random titles should not be the same:

```{r}

#random_matches_title_similarity <- dbGetQuery(con, read_file("sql/random_matches_title_similarity.sql"))

random_matches_title_similarity %>%
 select(EDIT_DISTANCE_SIMILARITY, JARO_WINKLER_SIMILARITY) %>%
  gather("EDIT_DISTANCE_SIMILARITY", "JARO_WINKLER_SIMILARITY", 
         key="method", 
         value="similarity") %>%
  group_by(method, similarity) %>%
  summarise(n = n()) %>%
  mutate(freq = n/sum(n)) %>%
  ggplot(aes(x=similarity, y=freq, fill=method, color=method)) +
  geom_bar(stat="identity") +
  labs(y="Percentage Matched", x="Title Similarity") +
  theme_bw() +
  theme(legend.position = "none") +
  scale_y_continuous(breaks=seq(from=0, to=1, by=0.1), labels=scales::percent) +
  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +
  coord_cartesian(xlim=c(0,100)) +
  facet_wrap(~method)

```

These plots show that the `EDIT_DISTANCE_SIMILARITY()` and `JARO_WINKLER_SIMILARITY()` produce much lower similarity scores when used to compare random titles, than when comparing titles of doi-matched articles. These results suggest that matching via string similarity functions can be an effective method to increase the number of matches whilst retaining high precision. Notably, we can see different distribution patterns between the two similarity measures - it appears that `EDIT_DISTANCE_SIMILARITY()` produces much lower similarity scores (median = `r median(random_matches_title_similarity$EDIT_DISTANCE_SIMILARITY`) with less overlap with the distribution of DOI-matched articles, compared to `JARO_WINKLER_SIMILARITY()` (median = `r median(random_matches_title_similarity$JARO_WINKLER_SIMILARITY`).

We can thus interpret that `EDIT_DISTANCE_SIMILARITY()` is a more appropriate measure for matching titles. For the remainder of this report we have thus implemented the `EDIT_DISTANCE_SIMILARITY()` function, with a similarity threshold of 80 %, for matching of article titles.

<!-- We use a threshold of 80 %, which captures XX % of all doi-matched articles -->

### Article title length

```{r}

# DOI-matched articles
#doi_matches_title_length <- dbGetQuery(con, read_file("sql/doi_matches_title_length.sql"))

doi_matches_title_length %>%
  rename(COUNT = "COUNT(*)") %>%
  ggplot(aes(x=DIFF, y=COUNT)) +
  geom_bar(stat="identity") +
  coord_cartesian(xlim=c(0,100))

```

```{r}

# Randomly matched articles
#random_matches_title_length <- dbGetQuery(con, read_file("sql/random_matches_title_length.sql"))

random_matches_title_length %>%
  rename(COUNT = "COUNT(*)") %>%
  ggplot(aes(x=DIFF, y=COUNT)) +
  geom_bar(stat="identity") +
  coord_cartesian(xlim=c(0,100))

```

### Article title blacklist/duplicates/something

<!-- Take a sample, show that there are duplicate titles which should be excluded. Show why a keyword blacklist is necessary -->


### Journal Titles 

<!-- Coverage -->

```{r}

# Coverage of journal titles in WOS and Unpaywall

wos_journal_coverage <- dbGetQuery(con, read_file("sql/wos_journal_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

upw_journal_coverage <- dbGetQuery(con, read_file("sql/upw_journal_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

```

<!-- Matching -->

```{r}

# Calculate percentage of doi-matched articles with exactly matching titles

doi_matches_exact_journals <- dbGetQuery(con, read_file("sql/doi_matches_exact_journals.sql")) %>%
  pull(PCT_EXACT_JOURNAL_MATCHES)

```

### ISSNs

<!-- Coverage -->

```{r}

# Coverage of journal titles in WOS and Unpaywall

wos_issn_coverage <- dbGetQuery(con, read_file("sql/wos_issn_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

upw_issn_coverage <- dbGetQuery(con, read_file("sql/upw_issn_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

```

<!-- Matching. Note Unpaywall ISSNs are separated in another table - maybe need to document in preprocessing section -->

```{r}

# Calculate percentage of doi-matched articles with exactly matching ISSNS

doi_matches_exact_issns <- dbGetQuery(con, read_file("sql/doi_matches_exact_issns.sql")) %>%
  pull(PCT_EXACT_ISSN_MATCHES)

```

### Author counts

<!-- Coverage -->

```{r}

wos_authorcount_coverage <- dbGetQuery(con, read_file("sql/wos_authorcount_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

upw_authorcount_coverage <- dbGetQuery(con, read_file("sql/upw_authorcount_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

```

<!-- Matching -->

```{r}

doi_matches_exact_authorcounts <- dbGetQuery(con, read_file("sql/doi_matches_exact_authorcounts.sql")) %>%
  pull(PCT_EXACT_AUTHORCOUNT_MATCHES)

```

<!-- Graph showing difference in WoS and Unpaywall author counts-->

```{r}

doi_matches_authorcounts <- dbGetQuery(con, read_file("sql/doi_matches_all_authorcounts.sql"))

doi_matches_authorcounts %>%
  ggplot(aes(x=UNPAYWALL_AUTHORS, y=WOS_AUTHORS)) +
  geom_tile(aes(fill=MATCHES)) +
  labs(x="Unpaywall Author Count", y="WoS Author Count") +
  scale_x_continuous(limits=c(0,100)) +
  scale_y_continuous(limits=c(0,100)) +
  scale_fill_viridis() +
  theme_bw()

doi_matches_authorcounts %>%
  mutate(difference = UNPAYWALL_AUTHORS-WOS_AUTHORS) %>%
  group_by(difference) %>%
  summarize(sum_matches = sum(MATCHES)) %>%
  filter(difference > -10, difference < 10) %>%
  ggplot(aes(x=difference, y=sum_matches)) +
  geom_bar(stat='identity')

```

### Author names

<!-- Coverage -->

```{r}


# Need to check! This doesn't work - null values can be double counted.

wos_first_author_name_coverage <- dbGetQuery(con, read_file("sql/wos_first_author_name_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

wos_last_author_name_coverage <- dbGetQuery(con, read_file("sql/wos_last_author_name_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

upw_first_author_name_coverage <- dbGetQuery(con, read_file("sql/upw_first_author_name_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

upw_last_author_name_coverage <- dbGetQuery(con, read_file("sql/upw_last_author_name_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

```

<!-- Matching -->

```{r}





```



## Data preprocessing

For matching of records between WoS and Unpaywall, a necessary first step is to clean and normalise the data, both to improve matching efficiency and performance of SQL queries. For example, when matching on the DOI field, it is necessary that both DOIs are in the same letter case, and do not contain any superfluous characters (e.g. leading and trailing whitespaces), otherwise matches may be missed.

For both Unpaywall and WoS we applied the following cleaning and normalisation procedures:

DOIs: Converted to lowercase (Oracle `LOWER()` function) and trimmed leading and trailing whitespace (Oracle `TRIM()` function)
Article titles: Converted to lowercase and trimmed leading and trailing whitespace
Journal titles: Converted to lowercase and trimmed leading and trailing whitespace

<!-- not needed anymore?

Author names: Author names were 'blocked' by concatenating the first initial of the first name, and the full last name (following the approach of e.g. Caron and Van Eck, 2014 #Ref list? , for large-scale author disambiguation). Author names were first converted to lowercase and leading and trailing whitespace was trimmed. A REGEX function was used to remove special characters from last names prior to concatenation (in Oracle: `REGEXP_REPLACE(NAME, '[^A-Za-z]','')`).-->

As mentioned before, we restricted the analysis to document types `journal-article` (Unpaywall) or `article` and `review` (WoS). From the imported Unpaywall table, and the WoS-KB database, we retrieved the fields we found to be potentially usefull in the previous section, namely:

- DOI
- Publication year
- Article title
- Length of the article title
- Author count
- Journal name
- Journal ISSNs

As shown in the previous section on potential matching criteria, both datasets contain articles titles which occur in multiple rows. We decided to exclude all of these titles from our matching candidates since we want to think of an article's title as unique for this publication. We also exluded all titles starting with one of the keywords contained in [this list](title_keyword_blacklist.csv), e.g. "correction", "erratum", or "addendum" since they would otherwise wrongly be matched with the original articles.

In order to speed up matching we created an index for the DOI column in both tables.


```{r}
#Prepare Unpaywall tables for matching, only run once

# dbGetQuery(con, read_file("sql/preprocessing/create_upw_14_norm.sql"))
# dbGetQuery(con, read_file("sql/preprocessing/create_upw_14_norm_index.sql"))
# dbGetQuery(con, read_file("sql/preprocessing/create_upw_14_issns.sql"))

#Prepare WoS tables for matching, only run once

# dbGetQuery(con, read_file("sql/preprocessing/create_wos_14_16_norm.sql"))
# dbGetQuery(con, read_file("sql/preprocessing/create_wos_14_16_norm_index.sql"))

```


## Matching algorithm

<!--(Anne/Nick)
Description of algorithm-->

The first step of our matching routine is simply using DOIs where they exist and are present in both databases. The more interesting part is the matching of articles without DOI information based on the criteria discussed in the previous section. As mentioned before, we only consider `journal article`s from Unpaywall and `articles`s and `review`s from WoS. We only try to match articles where the publication year from WoS is the same as or up to two years after the one in Unpaywall. Since all Unpaywall articles we investigated in this analysis are from 2014, we perform the matching on WoS articles with publication year between 2014 and 2016.



Since the comparison of titles to determine their similarity measure is costly, we try to narrow down the subset of possible matching candidates as far as possible using the following criteria:

- pubyear: As mentioned, before we require the publication year in WoS to be the same as or up to two years after the Unpaywall publication year. This is already implemented in the preprocessing of the two matching tables.
- journal information: We restrict matching candidates to records with the same ISSN or (exactly) the same journal title. When multiple ISSNs are associated with an article (e.g. of the print and electronic version of the journal), we compare to all of them.
- author count: We compare only articles with coinciding number of authors.
- length of article title: We compare only articles where the lengths of the article titles differ no more than 10 characters.

Then, we classify as matched articles the ones which have a title similarity (edit distance) of more than 80 percent.

This results in the query stored as [create_upw14_wos_matching_results.sql](create_upw14_wos_matching_results.sql) to obtain a list of matchings.

To evaluate the performance of our matching algorithm, we also run the matching algorithm on the two matching tables without using any DOI information (see the query [create_upw_14_wos_matching_results_eval.sql](create_upw_14_wos_matching_results_eval.sql)).

## Results

## Discussion and Recommendations

### Runtime

Preprocessing the data to generate the normalized and cleaned up tables `upw_14_norm` and `wos_14_16_norm` takes several minutes each.

# Bibliography

