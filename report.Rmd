---
title: "Report on Matching of Unpaywall and Web of Science"
output:
  pdf_document: default
  html_document:
    fig_caption: yes
    keep_md: yes
authors:
- Nicholas Fraser
- Anne Hobert
---

```{r setup, include=FALSE}

# knitr options
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      options(scipen=999))

# Call libraries
library(tidyverse)
library(RJDBC)
library(DBI)
library(dbplyr)
library(viridis)
library(scales)

# Store also personal authentication credentials as kb_user01 and kb_password01 in .Renviron file.

# Open DB connection

drv <-JDBC("oracle.jdbc.OracleDriver", classPath= "jdbc_driver/ojdbc8.jar")
con <- dbConnect(drv, "jdbc:oracle:thin:@//biblio-p-db01:1521/bibliodb01.fiz.karlsruhe", Sys.getenv("kb_user01"), Sys.getenv("kb_password01"))

```

## Summary

Combining data from multiple bibliometric databases requires accurate cross-matching of records. A potential way to match between databases is through the use of persistent identifiers, such as DOIs. However, in some cases DOI information is not available, which makes other matching methodologies necessary. Here we report a first attempt to match records between the Web of Science (WoS) in-house database of the German Competence Center for Bibliometrics (KB) and Unpaywall. Such a matching procedure is necessitated by the large amount of records in WoS missing DOI information. Our method relies on a relatively basic set of features available in both databases, including article titles, journal names, journal ISSNs, publication years, and author counts. Using these features, we applied a matching algorithm to a subset of Unpaywall and WoS data. Initial results are promising, with a matching precision of ~95 %  and a recall of ~99 % when compared to a 'gold standard' of articles matched by DOIs. Further improvement to the methodology may be possible through incorporation of further article and/or author features; however, scaling our process to be applied to the full WoS and Unpaywall databases remains a challenge.

## Motivation

This report documents our approach to developing a procedure for the connection of items covered by the Web of Science (WoS) in-house database of the German Competence Center for Bibliometrics (KB), to information on their open access status contained in Unpaywall.

The most straight-forward way to perform matching between WoS and Unpaywall is based on [digital object identifiers (DOI)](https://www.doi.org/), i.e. persistent interoperable identifiers. However, DOI information in WoS is incomplete: not all records have a DOI stored in the database even though many of them are actually included in Unpaywall.

```{r}
if(file.exists("data/wos_doi_coverage.csv")){
  wos_doi_cov <- read_csv("data/wos_doi_coverage.csv")
} else {
  wos_doi_cov <- dbGetQuery(con, read_file("sql/motivation/wos_doi_coverage.sql"))
  write_csv(wos_doi_cov, "data/wos_doi_coverage.csv")
}
wos_doi_cov_tidy <- wos_doi_cov %>%
  gather("is_null", "number_of_items", -TOTAL_NUMBER_OF_ITEMS, -ARTICLE_TYPE, -PUBYEAR) %>%
  mutate(is_null = case_when(
    is_null == "NUMBER_OF_NULLS" ~ TRUE,
    is_null == "NUMBER_OF_DOIS" ~ FALSE
  )) %>%
  select(-TOTAL_NUMBER_OF_ITEMS)

# Total number of null DOIs per year
wos_doi_cov_tidy %>%
  group_by(PUBYEAR, is_null) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  ggplot(aes(x = PUBYEAR, y = number_of_items)) + 
    geom_bar(aes(fill = is_null), stat = "identity", position = "dodge") +
    labs(x = "Publication year", y = "Number of items", fill = "DOI is null?",
         title = "How many items have a null-DOI in WoS?")
```
Looking at all `r wos_doi_cov_tidy %>% summarise(n = sum(number_of_items)) %>% .$n` items in the 2019 version of the WoS-KB bibliometric database (`wos_b_2019`) we see that only `r wos_doi_cov_tidy %>% filter(is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n` of them contain DOI information, corresponding to `r round((wos_doi_cov_tidy %>% filter(is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %.  The figure reveals that almost all of the items in WoS with a publication year before 2000 are missing any DOI information. Indeed, of `r wos_doi_cov_tidy %>% filter(PUBYEAR <2000) %>% summarise(n = sum(number_of_items)) %>% .$n` items older than 2000 in total, only `r wos_doi_cov_tidy %>% filter(PUBYEAR <2000, is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n` contain DOI information, corresponding to about `r round((wos_doi_cov_tidy %>% filter(PUBYEAR <2000, is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(PUBYEAR <2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %. In more recent years, the number of items including DOI information has risen sharply. The coverage is much better here, with `r round((wos_doi_cov_tidy %>% filter(PUBYEAR >= 2000, is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(PUBYEAR >= 2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` % of items published from 2000 onwards having DOI information. The decline in the number of items with and without DOI in the most recent 3 years is probably due to an indexing lag.

```{r}
# Number of null DOIs within articles and reviews per year
wos_doi_cov_tidy %>%
  filter(ARTICLE_TYPE %in% c("article", "review")) %>%
  group_by(PUBYEAR, is_null) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  ggplot(aes(x = PUBYEAR, y = number_of_items)) + 
    geom_bar(aes(fill = is_null), stat = "identity", position = "dodge") +
    labs(x = "Publication year", y = "Number of articles", fill = "DOI is null?",
         title = "How many articles and reviews have null-DOI in WoS?")
```

The observed trend is similar if we focus on the document types journal 'articles' and 'reviews' only. Overall, `r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review")) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` % of all articles and reviews contain DOI information. Again, the percentage is extremely low for publication years before 2000 (`r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE, PUBYEAR < 2000) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), PUBYEAR < 2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %), and higher for recent years (`r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE, PUBYEAR >= 2000) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), PUBYEAR >= 2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %). Moreover, for these document types, we observe that the number of items missing DOI information decreases continuously from 2000 onwards, leading to a coverage of `r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE, PUBYEAR == 2017) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), PUBYEAR == 2017) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` % in 2017.

We now want to investigate more thoroughly how the proportion of items without DOI depends on the article type. In order to keep the figures readable, we first identify the most common article types and collate the remaining ones in the category `Other`. Looking at the number of items per article type in the following figures, we decide to keep all types with more than `100,000` items.

```{r}
wos_doi_cov_tidy %>%
  group_by(ARTICLE_TYPE) %>%
  summarise(n = sum(number_of_items)) %>%
  arrange(desc(n)) %>%
  mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE)) %>%
  ggplot(aes(ARTICLE_TYPE, n)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(x = "Article type", y = "Number of items", title = "Total number of items per article type")

# wos_doi_cov_tidy %>%
#   group_by(ARTICLE_TYPE) %>%
#   summarise(n = sum(number_of_items)) %>%
#   arrange(desc(n)) %>%
#   mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE)) %>%
#   ggplot(aes(ARTICLE_TYPE, n)) +
#     geom_bar(stat = "identity") +
#     scale_y_log10() +
#     coord_flip() +
#     labs(x = "Article type", y = "Number of items", title = "Total number of items per article type, logarithmic scale")
```

With this, we see the following behavior for the existence of a DOI per article type.

```{r}
# Number of null DOIs per article type
# TODO: maybe make this a relative chart?
wos_doi_cov_tidy_factors <- wos_doi_cov_tidy %>%
  mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE))%>%
  group_by(ARTICLE_TYPE) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  arrange(desc(number_of_items)) %>%
  mutate(article_type_grouped = fct_reorder(fct_relevel(fct_other(ARTICLE_TYPE, keep = ARTICLE_TYPE[.$number_of_items > 0.01*sum(wos_doi_cov_tidy$number_of_items)]), "Other"), number_of_items))
wos_doi_cov_tidy %>%
  mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE))%>%
  mutate(article_type_grouped = fct_relevel(fct_other(ARTICLE_TYPE, keep = wos_doi_cov_tidy_factors$article_type_grouped), "Other")) %>%
  group_by(article_type_grouped, is_null) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  arrange(desc(number_of_items)) %>%
  ggplot(aes(x = fct_relevel(fct_reorder(article_type_grouped, desc(number_of_items)), "Other", after = Inf), y = number_of_items)) + 
    geom_bar(aes(fill = is_null), stat = "identity", position = "dodge") +
    coord_flip() + 
    labs(x = "Publication year", y = "Number of items", fill = "DOI is null?",
         title = "What article types have null-DOI in WoS?")
```
In this figure we see that missing DOIs are an issue particularly for proceedings papers, meeting abstracts, book reviews and marginal categories (`Other`). The proportion of journal articles and reviews without DOI is much lower, as the above results already indicated.

## Data selection and acquistion

To develop and test our matching strategy we decided to focus on data from Unpaywall with publication year 2014 to reduce storage space needed (the whole Unpaywall data dump needs more than 100 GB). For the most recent years, indexing lags often cause problems and, as just mentioned, for earlier years the DOI coverage within WoS is much worse.

We further decided to focus exlusively on journal articles and reviews since these are the article types covered best in WoS <!-- insert reference--> and we consider them to be most relevant for the studies and scenarios where our matching strategy might be applied.

This means that from the Unpaywall data we only consider records with publication year 2014 that have the `genre` of `journal-article`. 

We obtained our set of Unpaywall articles in a similar way to the method described in our [blog post on open access evidence in Unpaywall](https://subugoe.github.io/scholcomm_analytics/posts/unpaywall_evidence/). We will outline the main steps again below.

First, we downloaded the most recent Unpaywall data dump released in April 2019 from [here](https://s3-us-west-2.amazonaws.com/unpaywall-data-snapshots/) and imported it into a local MongoDB database. The snapshot contains more than 100 million records and hence, the file is about 100 GB in size To obtain a more manageable object to work with, we extracted the fields which are most relevant to our matching objective using the following query:

```{bash, eval = FALSE, echo = TRUE}
"C:\Program Files\MongoDB\Server\4.0\bin\mongo.exe"
db.unpaywallApr19.aggregate([
   {
      $match: {
          year: { $gte: 2014, $lte: 2016}
      }
   },
   {
      $project: {
          doi: 1,
          year: 1,
          genre: 1,
          title: 1,
          AuthorCount: { $cond: { if: { $isArray: "$z_authors" }, then: { $size: "$z_authors" }, else: null} },
          AuthorFirst: { $cond: { if: { $isArray: "$z_authors" }, then: { $arrayElemAt: [ "$z_authors", 0 ] }, else: null } },
          AuthorLast: { $cond: { if: { $isArray: "$z_authors" }, then: { $arrayElemAt: [ "$z_authors", -1 ] }, else: null } }
      }
   },
   {
      $out: 'authorInformation' 
   }
] )
```

Note that on a Linux or Mac OS, single quotes (`'`) and double quotes (`"`) need to be exchanged. We initially filtered for publication years between 2014 and 2016, extracted DOI, publication year, article type (`genre`), article title, and some author information, namely the number of authors, and the given and family names of the first and last author. For single author articles, the first and last author coincide. We exported the resulting data as json file and loaded it as table into our [GoogleBigQuery](https://cloud.google.com/bigquery/) analytical environment (an access protected paid service), specifying a [schema](schema_bigquery.json). There, we added information on the journal (ISSN and journal title) to the data from a previous export based on the local MongoDB database. We joined the journal related fields on DOI, which was possible since every Unpaywall entry contains DOI information. We then exported the table as three different .csv files, one for each publication year and imported the one corresponding to the publication year 2014 into the oracle environment of the KB.

## Matching criteria

A number of features may be appropriate for matching articles between bibliographic databases. In the best case scenario, persistent identifiers such as DOIs can be used for direct matching. In the cases where persistent identifiers are missing, other characteristics of an article and its authorship may be used for matching, such as the article title, journal title or ISSNs. However, none of these features are 100 % unique (e.g. articles can share titles or journal names) and thus using them in isolation may lead to unreliable matches. A combination of these characteristics can produce more precise matching, i.e. records contained in two databases with a number of positive matching features have a high probability of referring to the same article.

In this report we focus on an initial subset of characteristics which may potentially be used for matching between WoS and Unpaywall, including publication years, article titles, journal names, ISSNs, and author counts. These criteria were selected primarily due to data availability and coverage in both databases, but are not exhaustive and may be supplemented with further characteristics in future (e.g. page numbers). 

A description of the selected matching criteria follows. Note that for the following sections, items in WoS are limited only to 'article' and 'review' document types, and Unpaywall items to 'journal-article' types.

### Publication years

An initial way to filter potentially matching records is via their registered publication years in the two datasets. However, the registered publication dates in WoS and Unpaywall sometimes differ (for example, because one takes the online publication date and the other the print publication date). 

To understand how publication years vary between records contained in WoS and Unpaywall, we can consider the difference in publication years for all articles than can be directly matched with a DOI - in this way, we can refer to the DOI-matched articles as a 'gold standard' against which matching criteria (to be used for matching items without DOIs in WoS) can be investigated.

Below we show the number of items that can be matched between our Unpaywall dataset (publication year = 2014) and all items in WoS, as a function of the WoS publication year:

```{r}

# DOI matches as a function of WOS publication year (Unpaywall publication year = 2014).

if(file.exists("data/criteria/publication_years_doi_matches.csv")){
  publication_years_doi_matches <- read_csv("data/criteria/publication_years_doi_matches.csv")
} else {
  publication_years_doi_matches <- dbGetQuery(con, read_file("sql/criteria/publication_years_doi_matches.sql"))
  write_csv(publication_years_doi_matches, "data/criteria/publication_years_doi_matches.csv")
}

publication_years_doi_matches %>%
  ggplot(aes(x=WOS_YEAR, y=MATCHES)) +
  geom_bar(stat="identity") +
  labs(x="WOS Publication Year", y="Number of DOI matched items", title = "WoS publication years for items matched to Unpaywall (publication year = 2014") +
  theme_bw()

```

The figure shows that most items have the same publication year, 2014, in WoS as in the Unpaywall dataset. However, a non-negligible proportion of items have publication years one or two years later in WoS than in Unpaywall. Based on this, we recommend to focus try to match items in Unpaywall with items from WoS having the same publication year, or one or two years later.

### Article titles

```{r}

# Coverage of titles, and proportion of unique titles in WoS and Unpaywall

article_title_wos_coverage <- dbGetQuery(con, read_file("sql/criteria/article_title_wos_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
article_title_upw_coverage <- dbGetQuery(con, read_file("sql/criteria/article_title_upw_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
article_title_wos_uniques <- dbGetQuery(con, read_file("sql/criteria/article_title_wos_uniques.sql")) %>% 
  pull(PCT_UNIQUE)
article_title_upw_uniques <- dbGetQuery(con, read_file("sql/criteria/article_title_upw_uniques.sql")) %>% 
  pull(PCT_UNIQUE)

```

Article titles are potentially a good candidate for matching due to their high coverage: `r article_title_wos_coverage` % of 'article' and 'review' type documents in WoS, and `r article_title_upw_coverage` % of 'journal-article' type documents in Unpaywall have a title. However, when considering the proportion of unique article titles, `r article_title_wos_uniques` % of 'article' and 'review' titles in WoS are unique, compared to `r article_title_upw_uniques` of 'journal-articles' in Unpaywall, suggesting that relying on matching of titles alone would lead to a high number of false-positive matches.

```{r}

# Calculate percentage of doi-matched articles with exactly matching titles

article_title_doi_matches_exact <- dbGetQuery(con, read_file("sql/criteria/article_title_doi_matches_exact.sql")) %>%
  pull(PCT_EXACT_TITLE_MATCHES)

```

As with the publication years, we used the set of DOI-matched articles as a 'gold standard', and investigated the percentage of articles that have exactly matching titles. We find that only `r article_title_doi_matches_exact` % of DOI-matched articles have exactly matching titles. Reasons for this may be due to differences in character encoding, handling of special character, or sentence cases (i.e. upper vs lower-case) of titles processed by WoS and Unpaywall.

An alternative to exact matching is to consider approximate string matching, which would allow titles to be matched even if they possessed minor differences in a small number of characters. Oracle provides two built-in functions as part of the `UTL_MATCH` package for approximate string matching: `EDIT_DISTANCE_SIMILARITY()` AND `JARO_WINKLER_SIMILARITY()`. Both functions provide a similarity score that is normalised such that 1 equates to perfect similarity and 0 to no similarity (or 100 % and 0 % when given as percentages, respectively). `EDIT_DISTANCE_SIMILARITY()` provides a similarity score based upon the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two strings, which counts the number of edits (insertions, deletions, or substitutions) needed to convert one string to the other. `JARO_WINKLER_SIMILARITY()` provides a similarity score based on the [Jaro-Winkler distance](https://en.wikipedia.org/wiki/Jaro-Winkler_distance), which measures the number of characters in common and number of transpositions.

Below we plot a histogram of showing the proportion of matching titles in our DOI-matched set, as a function of the two similarity measures. To further improve the matching efficiency, we transform the title strings to lower case (using the Oracle `LOWER` function), and remove whitespace from the start and end of the string (using the Oracle `TRIM` function). Note that for performance reasons, we use a small sample of the Unpaywall results set (~1%, using the Oracle `SAMPLE(1)` function) for this initial testing phase:

```{r}

if(file.exists("data/criteria/article_title_doi_matches_similarity.csv")){
  article_title_doi_matches_similarity <- read_csv("data/criteria/article_title_doi_matches_similarity.csv")
} else {
  article_title_doi_matches_similarity <- dbGetQuery(con, read_file("sql/criteria/article_title_doi_matches_similarity.sql"))
  write_csv(article_title_doi_matches_similarity, "data/criteria/article_title_doi_matches_similarity.csv")
}

article_title_doi_matches_similarity %>%
  select(EDIT_DISTANCE_SIMILARITY, JARO_WINKLER_SIMILARITY) %>%
  gather("EDIT_DISTANCE_SIMILARITY", "JARO_WINKLER_SIMILARITY", 
         key="method", 
         value="similarity") %>%
  group_by(method, similarity) %>%
  summarise(n = n()) %>%
  mutate(freq = n/sum(n)) %>%
  ggplot(aes(x=similarity, y=freq, fill=method, color=method)) +
  geom_bar(stat="identity") +
  labs(y="Percentage of Articles Matched", x="Title Similarity") +
  theme_bw() +
  theme(legend.position = "none") +
  scale_y_continuous(breaks=seq(from=0, to=1, by=0.1), labels=scales::percent) +
  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +
  facet_wrap(~method)

```

These plots show that `EDIT_DISTANCE_SIMILARITY()` and `JARO_WINKLER_SIMILARITY()` have similar distributions of similarity scores for titles on DOI-matched articles. Both distributions show that titles are consistently matched with similarities > 80 % (with a small percentage of outliers), suggesting this as a suitable threshold below which matched titles should be considered unreliable.

An additional method to assess the efficacy of the different similarity matching methods is to assess matching similarity on randomly matched articles, i.e. assuming that random titles should not be the same:

```{r}

if(file.exists("data/criteria/article_title_random_matches_similarity.csv")){
  article_title_random_matches_similarity <- read_csv("data/criteria/article_title_random_matches_similarity.csv")
} else {
  article_title_random_matches_similarity <- dbGetQuery(con, read_file("sql/criteria/article_title_random_matches_similarity.sql"))
  write_csv(article_title_random_matches_similarity, "data/criteria/article_title_random_matches_similarity.csv")
}

article_title_random_matches_similarity %>%
 select(EDIT_DISTANCE_SIMILARITY, JARO_WINKLER_SIMILARITY) %>%
  gather("EDIT_DISTANCE_SIMILARITY", "JARO_WINKLER_SIMILARITY", 
         key="method", 
         value="similarity") %>%
  group_by(method, similarity) %>%
  summarise(n = n()) %>%
  mutate(freq = n/sum(n)) %>%
  ggplot(aes(x=similarity, y=freq, fill=method, color=method)) +
  geom_bar(stat="identity") +
  labs(y="Percentage Matched", x="Title Similarity") +
  theme_bw() +
  theme(legend.position = "none") +
  scale_y_continuous(breaks=seq(from=0, to=1, by=0.1), labels=scales::percent) +
  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +
  coord_cartesian(xlim=c(0,100)) +
  facet_wrap(~method)

```

These plots show that the `EDIT_DISTANCE_SIMILARITY()` and `JARO_WINKLER_SIMILARITY()` produce much lower similarity scores when used to compare random titles, than when comparing titles of doi-matched articles. These results suggest that matching via string similarity functions can be an effective method to increase the number of matches whilst retaining high precision. Notably, we can see different distribution patterns between the two similarity measures - it appears that `EDIT_DISTANCE_SIMILARITY()` produces much lower similarity scores (median = `r median(article_title_random_matches_similarity$EDIT_DISTANCE_SIMILARITY)` %) with less overlap with the distribution of DOI-matched articles, compared to `JARO_WINKLER_SIMILARITY()` (median = `r median(article_title_random_matches_similarity$JARO_WINKLER_SIMILARITY)` %).

We can thus interpret that `EDIT_DISTANCE_SIMILARITY()` is a more appropriate measure for matching titles. For matching between datasets we thus recommend to implement the `EDIT_DISTANCE_SIMILARITY()` function, with a similarity threshold of 80 %, for matching of article titles.

### Article title length

In the above section we matched titles using Oracle built-in functions for approximate string matching. However, such string similarity algorithms are notoriously slow on large datasets, as they require a matrix of rows to be checked against every other row in the dataset (e.g. if we were to compare 10,000 Unpaywall records with 10,000 WoS records, we would need to perform 100,000,000 string similarity comparisons). A target for scaling such similarity functions is to reduce the number of pairwise comparisons between titles. One potential way to do this is by first considering differences in the title length, where titles with greatly differing lengths are unlikely to belong to the same record.

First we compare the difference in title lengths of Unpaywall and WoS items in our DOI-matched dataset:

```{r}

if(file.exists("data/criteria/article_title_doi_matches_length.csv")){
  article_title_doi_matches_length <- read_csv("data/criteria/article_title_doi_matches_length.csv")
} else {
  article_title_doi_matches_length <- dbGetQuery(con, read_file("sql/criteria/article_title_doi_matches_length.sql"))
  write_csv(article_title_doi_matches_length, "data/criteria/article_title_doi_matches_length.csv")
}

article_title_doi_matches_length %>%
  ggplot(aes(x=LENGTH_DIFFERENCE, y=N)) +
  geom_bar(stat="identity") +
  coord_cartesian(xlim=c(0,100)) +
  labs(x="Difference in Title Length", y="Number of Matches") +
  theme_bw()

```

For our From this, we can see that the vast majority of titles do not differ significantly in length: `r round(100*(article_title_doi_matches_length %>% filter(LENGTH_DIFFERENCE <= 10) %>% tally(N) %>% pull(n)) /(article_title_doi_matches_length %>% tally(N) %>% pull(n)), 2)` % of titles differ by less than 10 characters in length.

Another way to understand this is to compare the difference in length between titles of articles that are randomly matched, as, unlike our DOI-matched articles, we would expect randomly matched articles to have very different title lengths:

```{r}

if(file.exists("data/criteria/article_title_random_matches_length.csv")){
  article_title_random_matches_length <- read_csv("data/criteria/article_title_random_matches_length.csv")
} else {
  article_title_random_matches_length <- dbGetQuery(con, read_file("sql/criteria/article_title_random_matches_length.sql"))
  write_csv(article_title_random_matches_length, "data/criteria/article_title_random_matches_length.csv")
}

article_title_random_matches_length %>%
  ggplot(aes(x=LENGTH_DIFFERENCE, y=N)) +
  geom_bar(stat="identity") +
  coord_cartesian(xlim=c(0,100)) +
  labs(x="Difference in Title Length", y="Number of Matches") +
  theme_bw()

```

The results show that there exists a wide variability in the length of titles when randomly matched, thus, by only comparing titles with similar lengths, we could exclude a large number of pairwise similarity comparisons between article titles. For example, excluding articles with differences in title lengths over 10 characters, would exclude `r round(100*(article_title_random_matches_length %>% filter(LENGTH_DIFFERENCE >= 10) %>% tally(N) %>% pull(n)) /(article_title_random_matches_length %>% tally(N) %>% pull(n)), 2)` % of the pairwise comparisons required, greatly improving performance.

### Article title duplicates

A secondary factor related to titles is that of potential duplicates, i.e. two distinct records within the same database that share the same title. In some cases, title names can be shared by a large number of items (e.g. "editorial board"):

```{r}

# Unpaywall duplicate titles
if(file.exists("data/criteria/article_title_upw_duplicates.csv")){
  article_title_upw_duplicates <- read_csv("data/criteria/article_title_upw_duplicates.csv")
} else {
  article_title_upw_duplicates <- dbGetQuery(con, read_file("sql/criteria/article_title_upw_duplicates.sql"))
  write_csv(article_title_upw_duplicates, "data/criteria/article_title_upw_duplicates.csv")
}

# WoS duplicate titles
if(file.exists("data/criteria/article_title_wos_duplicates.csv")){
  article_title_wos_duplicates <- read_csv("data/criteria/article_title_wos_duplicates.csv")
} else {
  article_title_wos_duplicates <- dbGetQuery(con, read_file("sql/criteria/article_title_wos_duplicates.sql"))
  write_csv(article_title_wos_duplicates, "data/criteria/article_title_wos_duplicates.csv")
}

article_title_upw_duplicates %>%
  arrange(desc(N)) %>%
  top_n(10) %>%
  ggplot(aes(x=reorder(ARTICLE_TITLE, N), y=N)) +
  geom_bar(stat="identity") +
  coord_flip() +
  theme_bw() +
  labs(y="Number of articles", x="", title="Number of duplicate articles for common titles in\n Unpaywall (top 10)")

article_title_wos_duplicates %>%
  arrange(desc(N)) %>%
  top_n(10) %>%
  ggplot(aes(x=reorder(ARTICLE_TITLE, N), y=N)) +
  geom_bar(stat="identity") +
  coord_flip() +
  theme_bw() +
  labs(y="Number of articles", x="", title="Number of duplicate articles for common titles in\n WoS (top 10)") +
  scale_x_discrete( label = function(x) str_trunc(x, 30))

```

Due to these duplicate titles which cannot be definitively matched via title matching methods, our recommendation is to remove all items with duplicate titles from our datasets, and only match on records containing unique titles.

During these title matching processes, we also noticed a number of examples where items might match due to high similarity of titles, when they represent related (but not the same) documents. For example, an article may have a corrigendum associated with it, with the title "Corrigendum: [original article title]". Clearly, these two records do not represent the same document, but may match due to the similarity between titles, and matching of other associated metadata (e.g. the corrigendum may be in the same journal and have the same authors as the original document).

<!-- Need to add this - how did we generate manual list? -->

### Journal Titles 



```{r}

journal_title_wos_coverage <- dbGetQuery(con, read_file("sql/criteria/journal_title_wos_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
journal_title_upw_coverage <- dbGetQuery(con, read_file("sql/criteria/journal_title_upw_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

```

Journal titles present another possible facet for matching. First we checked the coverage of journal titles in datasets, finding that `r journal_title_wos_coverage` % of items in WoS, and `r journal_title_upw_coverage` % of items in Unpaywall are associated with a journal title.

```{r}
journal_title_doi_matches_exact <- dbGetQuery(con, read_file("sql/criteria/journal_title_doi_matches_exact.sql")) %>%
  pull(PCT_EXACT_JOURNAL_MATCHES)
```

We then checked the percentage of articles in our DOI-matched sample, that also possess the exact same journal title. We applied the Oracle functions `LOWER` and `TRIM` to the journal titles before matching, to improve efficiency. Despite a high level of coverage of titles in both databases, only `r journal_title_doi_matches_exact` % of journal titles match exactly, for articles that can be matched together via DOIs. The reason for this is either minor changes in journal titles between datasets due to character encoding issues, or inconsistent reporting of journal titles between databases (e.g "PNAS" versus "Proceedings of the National Academy of Sciences").

### ISSNs

```{r}

issn_wos_coverage <- dbGetQuery(con, read_file("sql/criteria/issn_wos_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
issn_upw_coverage <- dbGetQuery(con, read_file("sql/criteria/issn_upw_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

```

A potential alternative to matching on journal titles is to match on the journal ISSN. As before, we first checked the coverage of ISSNs within each dataset, finding that `r issn_wos_coverage` % of items in WoS, and `r issn_upw_coverage` $ of items in Unpaywall possess ISSN information.

```{r}

issn_doi_matches_exact <- dbGetQuery(con, read_file("sql/criteria/issn_doi_matches_exact.sql")) %>%
  pull(PCT_EXACT_ISSN_MATCHES)

```

We then calculated the percentage of DOI-matched items that have a matching ISSN. Note that some records in Unpaywall have multiple ISSNs associated with them, whilst WoS records are only associated with a single ISSN. We thus match dependent on whether ANY of the ISSNs associated with an item in Unpaywall are the same as the ISSN for an item in WoS. Note that ISSNs in Unpaywall are parsed to a separate table (documented in the following section on data preprocessing). We found that `r issn_doi_matches_exact` % of DOI-matched articles also have a matching ISSN, a higher rate than for journal titles alone. To provide the greatest coverage possible, we therefore recommend to conduct matching based on the ISSN OR the journal title.

### Author counts

The final feature that we investigate here as a potential matching option is for author counts, i.e. matched articles should have the same number of authors. Author count data is calculated within the KB database directly, whilst author counts for Unpaywall are calculated from the length of the "z_authors" field (see above section on "Data selection and acquistion"). 

```{r}

authorcount_wos_coverage <- dbGetQuery(con, read_file("sql/criteria/authorcount_wos_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
authorcount_upw_coverage <- dbGetQuery(con, read_file("sql/criteria/authorcount_upw_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

```

First we checked the coverage of author counts in both datasets, finding that `r authorcount_wos_coverage` % of WoS items, and `r authorcount_upw_coverage` % of Unpaywall items had author counts.

```{r}

authorcount_doi_matches_exact <- dbGetQuery(con, read_file("sql/criteria/authorcount_doi_matches_exact.sql")) %>%
  pull(PCT_EXACT_AUTHORCOUNT_MATCHES)

```

We then checked  the number of DOI-matched articles that also have exactly matching number of authors, finding that 
`r authorcount_doi_matches_exact` % of articles that can be matched by DOIs also have matching numbers of authors. This high percentage means that we recommend to also use author counts as a matching feature.

## Data preprocessing

For matching of records between WoS and Unpaywall, a necessary first step is to clean and normalise the data, both to improve matching efficiency and performance of SQL queries. For example, when matching on the DOI field, it is necessary that both DOIs are in the same letter case, and do not contain any superfluous characters (e.g. leading and trailing whitespaces), otherwise matches may be missed.

For both Unpaywall and WoS we applied the following cleaning and normalisation procedures:

- DOIs: Converted to lowercase (Oracle `LOWER()` function) and trimmed leading and trailing whitespace (Oracle `TRIM()` function)
- Article titles: Converted to lowercase and trimmed leading and trailing whitespace
- Journal titles: Converted to lowercase and trimmed leading and trailing whitespace

As mentioned before, we restricted the analysis to document types `journal-article` (Unpaywall) or `article` and `review` (WoS). From the imported Unpaywall table, and the WoS-KB database, we retrieved the fields we found to be potentially useful in the previous section, namely:

- DOI
- Publication year
- Article title
- Length of the article title
- Author count
- Journal name
- Journal ISSNs

As shown in the previous section on potential matching criteria, both datasets contain articles titles which occur in multiple rows. We decided to exclude all of these titles from our matching candidates since we want to think of an article's title as unique for this publication. We also exluded all titles starting with one of the keywords contained in [this list](title_keyword_blacklist.csv), e.g. "correction", "erratum", or "addendum" since they would otherwise wrongly be matched with the original articles.

In order to speed up matching we also created an index for the DOI column in both tables.


```{r}
#Prepare Unpaywall tables for matching, only run once

# dbGetQuery(con, read_file("sql/preprocessing/create_upw_14_norm.sql"))
# dbGetQuery(con, read_file("sql/preprocessing/create_upw_14_norm_index.sql"))
# dbGetQuery(con, read_file("sql/preprocessing/create_upw_14_issns.sql"))

#Prepare WoS tables for matching, only run once

# dbGetQuery(con, read_file("sql/preprocessing/create_wos_14_16_norm.sql"))
# dbGetQuery(con, read_file("sql/preprocessing/create_wos_14_16_norm_index.sql"))

```


## Matching algorithm

The first step of our matching routine is simply using DOIs where they exist and are present in both databases. The more interesting part is the matching of articles without DOI information based on the criteria discussed in the previous section. As mentioned before, we only consider `journal article` document types from Unpaywall and `article` and `review` document types from WoS. We only try to match articles where the publication year from WoS is the same as or up to two years after the one in Unpaywall. Since all Unpaywall articles we investigated in this analysis are from 2014, we perform the matching on WoS articles with publication year between 2014 and 2016.

Since the comparison of titles to determine their similarity measure is costly, we try to narrow down the subset of possible matching candidates as far as possible using the following criteria:

- pubyear: As mentioned, before we require the publication year in WoS to be the same as or up to two years after the Unpaywall publication year. This is already implemented in the preprocessing of the two matching tables.
- journal information: We restrict matching candidates to records with the same ISSN or (exactly) the same journal title. When multiple ISSNs are associated with an article (e.g. of the print and electronic version of the journal), we compare to all of them.
- author count: We compare only articles with coinciding number of authors.
- length of article title: We compare only articles where the lengths of the article titles differ no more than 10 characters.

Then, we classify as matched articles the ones which have a title similarity (edit distance) of more than 80 percent.

This results in the query stored as [create_upw14_wos_matching_results.sql](create_upw14_wos_matching_results.sql) to obtain a list of matchings.

To evaluate the performance of our matching algorithm, we also run the matching algorithm on the two matching tables without using any DOI information (see the query [create_upw_14_wos_matching_res_eval.sql](create_upw_14_wos_matching_res_eval.sql)).

## Results

<!--how to compare total number of mtaches to number of articles to be matched? esp. how to compare to wos records? just 2014 wos and 2014 matched?-->

```{r}
#doi_matches <- dbGetQuery(con, read_file("sql/results/gather_matching_results.sql"))
if(file.exists("data/results/doi_matches_aggr.csv")){
  doi_matches_aggr <- read_csv("data/results/doi_matches_aggr.csv")
} else {
  doi_matches_aggr <- dbGetQuery(con, read_file("sql/results/gather_matching_results_aggr.sql"))
  write_csv(doi_matches_aggr, "data/results/doi_matches_aggr.csv")
}
```

Using the above described algorithm, a total of `r doi_matches_aggr$TOTAL_MATCHES` matched records were obtained. `r doi_matches_aggr$DOI_MATCHES` were matched by DOI, corresponding to `r round(doi_matches_aggr$DOI_MATCHES/doi_matches_aggr$TOTAL_MATCHES * 100, 2)` %. Of the remaining `r doi_matches_aggr$TOTAL_MATCHES - doi_matches_aggr$DOI_MATCHES` matches, for `r doi_matches_aggr$WOS_NULLS`, DOI was missing in WoS and `r doi_matches_aggr$DIFF_DOIS` had differing DOIs (i.e. the WoS DOI and the Unpaywall DOI were different for matched records). The following figure illustrates these numbers:

```{r}
doi_matches_aggr %>% 
  select(TOTAL_MATCHES, DOI_MATCHES, WOS_NULLS, DIFF_DOIS) %>% 
  gather(key = "doi_is_matching", value = "number_of_matches", -TOTAL_MATCHES) %>% 
  mutate(doi_is_matching = factor(doi_is_matching, levels = c("DOI_MATCHES", "WOS_NULLS", "DIFF_DOIS"))) %>% 
  mutate(doi_is_matching = fct_recode(doi_is_matching, "matching DOIs"= "DOI_MATCHES", "WoS DOI is null" = "WOS_NULLS", "differing DOIs" = "DIFF_DOIS")) %>% 
  ggplot(aes(x = doi_is_matching, y = number_of_matches)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = comma) +
  labs(x = "", y = "Number of matches", title = "Are DOIs of matched records the same?")
```

Some articles in Unpaywall were matched with multiple WoS records. A superficial check of these records revealed that for some of them, the same DOI was registered with differing items key and title in WoS. For others, the titles were almost identical with just one word or number changed (for example, "part 1" and "part 2"). They mostly appeared in the same journal with identical author count and were thus not recognized as different articles by the algorithm. The following figure shows the frequency of duplicates.

```{r}
doi_duplicates <- dbGetQuery(con, read_file("sql/results/gather_duplicates.sql"))
doi_duplicates %>% 
  ggplot(aes(x = N)) +
    geom_histogram(binwidth = 1) +
    labs(x = "Number of duplicates", y = "Number of articles", title = "How frequent are duplicate matches?")
```

We additionally took a sample of 100 matched articles, which did not have any DOI information from WoS but were still identified with a corresponding article in Unpaywall. We manually checked all corresponding Unpaywall DOIs and found that all of them resolved to the correct article. It seems that at least for our sample the DOI was just not registered with WoS.

<!--In a second approach, we ran the matching algorithm on our normalized tables without, however, using the DOI field. The following figure shows a comparison of the number of matched records from the two approaches, using DOI information or not, and highlights which of them have identical DOIs, differing DOIs or no DOI in WoS.

```{r}
if(file.exists("data/results/doi_matches_eval_aggr.csv")){
  doi_matches_eval_aggr <- read_csv("data/results/doi_matches_eval_aggr.csv")
} else {
  doi_matches_eval_aggr <- dbGetQuery(con, read_file("sql/results/gather_matching_results_eval_aggr.sql"))
  write_csv(doi_matches_eval_aggr, "data/results/doi_matches_eval_aggr.csv")
}
```

```{r, eval = FALSE}
doi_matches_compare_plot 
```
-->

### Evaluation of matching routine

To evaluate the performance of our matching algorithm we considered the set of DOI-matched articles as the ground truth, that is we assume that records with matching DOIs are referring to the same article, and articles without a DOI match in the other dataset are only registered in one of the databases. Therefore, in the analysis, we will consider only articles that have DOI information also in WoS, since they could potentially be matched by DOI and exclude all articles with null DOI. 

Comparing the number of articles that were matched by our algorithm in the second approach with the ones matched by DOI, we can calculate precision and recall. We use distinct counting here, we assume multiple records with the same DOI to be rather a problem of duplicate database entries than of incorrect matching.

```{r}
n_matches_eval <- dbGetQuery(con, read_file("sql/results/gather_number_matches_eval.sql"))
n_matches_doi <- dbGetQuery(con, read_file("sql/results/gather_number_matches_doi.sql"))
```

We find that using purely DOI, a total number of `r n_matches_doi$N` articles could be matched. Our algorithm found `r n_matches_eval$TOTAL_MATCHES` in total, of which `n_matches_eval$DOI_MATCHES` have coinciding DOIs. This corresponds to a recall of `r round(n_matches_eval$DOI_MATCHES/n_matches_doi$N*100, 2)` % and a precision of `r round(n_matches_eval$DOI_MATCHES/n_matches_eval$TOTAL_MATCHES*100, 2)` %. This means that even without using DOI information, almost 95 % percent of articles that could be matched by DOI can be matched correctly at a mismatch-rate of less than 1 %.

<!-- figure?-->

## Discussion and Recommendations

In general, the results are quite promising. The algorithm is able to retrieve a large number of records even without using DOI information. For the more recent years with a better DOI coverage, it still finds additional matches for the entries with missing DOI. The amount of mismatches is quite low. However, there are some critical points that need to be considered and addressed in some way for an implementation of the algorithm for the purposes of the KB.

Firstly, the proposed algorithm can not distinguish well between articles with very similar titles published in the same journal and year with the same authorcount. For example, titles that only differ in a number (part 1, part 2, and so on) or a year or a single word are typically matched, leading to several uncorrect additional matches. Since the number of these occurances is quite low, we decided not to take further steps to exclude them. It might be possible to reduce the number of mismatches further by using more matching criteria, like authornames or to choose a very high threshold for the title similarity at the risk of loosing some other matches.

A second point to consider is that we decided to exclude all records that do not have unique titles in the preprocessing step. This might be too restrictive. Excluding only articles that coincide in the title and all other matching criteria will likely lead to an even higher number of matched articles. Alternatively, instead of excluding duplicate entries, one of them could be chosen.

In our algorithm, we require exactly matching authorcounts. For the Unpaywall dataset we caculated the authorcount ourselves from the nested field that lists all authors. For the WoS, however, we used the preimplemented authorcount from the items table without running any tests how reliable this number is (apart from comparing authorcounts with the ones from Unpwaywall for the DOI-matched records). Calculating the authorcounts from the available author information might yield better results.

Running the matching algorithm (without using DOI information) took little over 4 hours on the scriptserver. When using also DOI information, it is slightly faster, but also takes several hours to complete. For this report we only considered one year of Unpaywall records and three years of WoS records. Since the matching algorithm uses a restriction on certain publication years, we expect the runtime to scale more or less linearly if more publication years are included. Depending on the study one wants to conduct and the amount of data that should be included, this might not be feasible. Hence, for a single study, that focuses on more recent years, matching articles purely based on doi might be the best option.

To obtain more matches, especially for years before 2000, where the DOI coverage is unsatisfying, it might be advised to implement a more sophisticated algorithm. In our case the final matching criterion was a high title similarity. We use several other arguments to reduce the number of titles that have to be compared, since that is the most costly step in terms of run time. We found the following steps to be helpful:
 - Publication year: Compare Unpaywall entries to WoS entries with the same publication year or up to two years later
 - Journal: Compare articles with exactly the same journal title or matching ISSN
 - Authorcount: We allowed only exactly matching authorcounts.
 - Title length: We allowed for title lengths to differ up to 10 characters but not more.

For the integration of open access information into the KB infrastructure, a matching would most likely not have to be performed by each user individually, but could be done once after every upload of a new Unpaywall dump. Hence, an algorithm based on the one we developed for this report could be implemented to create a matching 

