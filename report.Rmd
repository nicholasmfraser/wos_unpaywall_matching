---
title: "Report on Matching of Unpaywall and Web of Science"
authors:
  - "Nicholas Fraser"
  - "Anne Hobert"
output:
  html_document:
    keep_md: TRUE
    fig_caption: TRUE
---
## Initial setup

```{r setup, include=FALSE}

# knitr options
knitr::opts_chunk$set(echo = TRUE)

# Call libraries
library(tidyverse)
library(RJDBC)
library(DBI)
library(dbplyr)
library(viridis)

# Store also personal authentication credentials as kb_user01 and kb_password01 in .Renviron file.

# Open DB connection

drv <-JDBC("oracle.jdbc.OracleDriver", classPath= "jdbc_driver/ojdbc8.jar")
con <- dbConnect(drv, "jdbc:oracle:thin:@//biblio-p-db01:1521/bibliodb01.fiz.karlsruhe", Sys.getenv("kb_user01"), Sys.getenv("kb_password01"))

```

## Motivation

<!-- There is a growing need to monitor open access to scholarly literature. Information on the open access status of research publications is desired to study the development of open availability over time, phenomena connected with open access, like a possible citation advantage, to observe compliance with funder mandates and to inform decision making, be it on institutional level or in science policy.

Yet, gathering information on the open access status often is not a trivial task.-->

This report documents our approach to developing a procedure for the connection of items covered by the Web of Science (WoS) in-house database of the Competence Center for bibliometrics (KB) to information on their open access status contained in the Unpaywall data dump.

The most straight-forward way to perform a matching between WoS and Unpwaywall is based on [digital object identifiers (doi)](https://www.doi.org/), i.e. persistent interoperable identifiers. However, doi information in WoS is incomplete: not for all records a doi is stored in the database even though many of them are actually included in Unpaywall.

```{r}
wos_doi_cov <- dbGetQuery(con, read_file("sql/wos_doi_coverage.sql"))
write_csv(wos_doi_cov, "data/wos_doi_coverage.csv")
wos_doi_cov_tidy <- wos_doi_cov %>%
  gather("is_null", "number_of_items", -TOTAL_NUMBER_OF_ITEMS, -ARTICLE_TYPE, -PUBYEAR) %>%
  mutate(is_null = case_when(
    is_null == "NUMBER_OF_NULLS" ~ TRUE,
    is_null == "NUMBER_OF_DOIS" ~ FALSE
  )) %>%
  select(-TOTAL_NUMBER_OF_ITEMS)

# Total number of null DOIs per year
wos_doi_cov_tidy %>%
  group_by(PUBYEAR, is_null) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  ggplot(aes(x = PUBYEAR, y = number_of_items)) + 
    geom_bar(aes(fill = is_null), stat = "identity", position = "dodge") +
    labs(x = "Publication year", y = "Number of items", fill = "DOI is null?",
         title = "What number of items have null-DOI in WoS?")
```
Looking at all `r wos_doi_cov_tidy %>% summarise(n = sum(number_of_items)) %>% .$n` items in the WoS-KB bibliometric database (`wos_b_2019`) we see that only `r wos_doi_cov_tidy %>% filter(is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n` of them contain DOI information, corresponding to `r round((wos_doi_cov_tidy %>% filter(is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %.  The figure reveals that almost all of the items in WoS with a publication year before 2000 are missing any DOI information. Indeed, of `r wos_doi_cov_tidy %>% filter(PUBYEAR <2000) %>% summarise(n = sum(number_of_items)) %>% .$n` items older than 2000 in total, only `r wos_doi_cov_tidy %>% filter(PUBYEAR <2000, is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n` contain DOI information, corresponding to about `r round((wos_doi_cov_tidy %>% filter(PUBYEAR <2000, is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(PUBYEAR <2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %. In more recent years, the number of items including DOI information rises continuously. The coverage is much better here, with `r round((wos_doi_cov_tidy %>% filter(PUBYEAR >= 2000, is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(PUBYEAR >= 2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` % of items published from 2000 onwards having DOI information. The decline in the number of items with and without doi in the last years is probably due to an indexing lag.

```{r}
# Number of null DOIs within articles and reviews per year
wos_doi_cov_tidy %>%
  filter(ARTICLE_TYPE %in% c("article", "review")) %>%
  group_by(PUBYEAR, is_null) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  ggplot(aes(x = PUBYEAR, y = number_of_items)) + 
    geom_bar(aes(fill = is_null), stat = "identity", position = "dodge") +
    labs(x = "Publication year", y = "Number of articles", fill = "DOI is null?",
         title = "What number of articles and reviews have null-DOI in WoS?")
```

The observed trend is similar if we focus on the document types journal articles and reviews only. Overall, `r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review")) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` % of all articles and reviews contain DOI information. Again, the percentage is extremely low for publication years before 2000 (`r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE, PUBYEAR < 2000) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), PUBYEAR < 2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %), and higher for recent years (`r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE, PUBYEAR >= 2000) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), PUBYEAR >= 2000) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` %). Moreover, for these document types, we observe that the number of items missing DOI information decreases continuously from 2000 onwards, leading to a coverage of `r round((wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), is_null == FALSE, PUBYEAR == 2017) %>% summarise(n = sum(number_of_items)) %>% .$n)/(wos_doi_cov_tidy %>% filter(ARTICLE_TYPE %in% c("article", "review"), PUBYEAR == 2017) %>% summarise(n = sum(number_of_items)) %>% .$n), 4)*100` % in 2017.

We now want to investigate more thoroughly how the proportion of items without doi depends on the article type. In order to keep the figures readable, we first identify the most common article types and collate the remaining ones in the category `Other`. Looking at the number of items per article type in the following figures, we decide to keep all types with more than `100,000` items.

```{r}
wos_doi_cov_tidy %>%
  group_by(ARTICLE_TYPE) %>%
  summarise(n = sum(number_of_items)) %>%
  arrange(desc(n)) %>%
  mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE)) %>%
  ggplot(aes(ARTICLE_TYPE, n)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(x = "Article type", y = "Number of items", title = "Total number of items per article type")

wos_doi_cov_tidy %>%
  group_by(ARTICLE_TYPE) %>%
  summarise(n = sum(number_of_items)) %>%
  arrange(desc(n)) %>%
  mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE)) %>%
  ggplot(aes(ARTICLE_TYPE, n)) +
    geom_bar(stat = "identity") +
    scale_y_log10() +
    coord_flip() +
    labs(x = "Article type", y = "Number of items", title = "Total number of items per article type, logarithmic scale")
```

With this, we see the following behavior for the existence of a doi per article type.

```{r}
# Number of null DOIs per article type
# TODO: maybe make this a relative chart?
wos_doi_cov_tidy_factors <- wos_doi_cov_tidy %>%
  mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE))%>%
  group_by(ARTICLE_TYPE) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  arrange(desc(number_of_items)) %>%
  mutate(article_type_grouped = fct_reorder(fct_relevel(fct_other(ARTICLE_TYPE, keep = ARTICLE_TYPE[.$number_of_items > 0.01*sum(wos_doi_cov_tidy$number_of_items)]), "Other"), number_of_items))
wos_doi_cov_tidy %>%
  mutate(ARTICLE_TYPE = as_factor(ARTICLE_TYPE))%>%
  mutate(article_type_grouped = fct_relevel(fct_other(ARTICLE_TYPE, keep = wos_doi_cov_tidy_factors$article_type_grouped), "Other")) %>%
  group_by(article_type_grouped, is_null) %>%
  summarise(number_of_items = sum(number_of_items)) %>%
  arrange(desc(number_of_items)) %>%
  ggplot(aes(x = fct_relevel(fct_reorder(article_type_grouped, desc(number_of_items)), "Other", after = Inf), y = number_of_items)) + 
    geom_bar(aes(fill = is_null), stat = "identity", position = "dodge") +
    coord_flip() + 
    labs(x = "Publication year", y = "Number of items", fill = "DOI is null?",
         title = "What article types have null-DOI in WoS?")
```
In this figure we see that missing DOIs are an issue particularly for proceedings papers, meeting abstracts, book reviews and marginal categories (`Other`). The proportion of journal articles and reviews without DOI is much lower, as the above results already indicated.

## Data selection and acquistion

To develop and test our matching strategy we decided to focus on data from Unpaywall with publication year 2014 to reduce storage space needed (the whole Unpaywall data dump needs more than 100 GB). For the most recent years often indexeing lags cause problems and, as just mentioned, for earlier years the DOI coverage within WoS is much worse.

We further decided to focus exlusively on journal articles and reviews since these are the article types covered best in WoS <!-- insert reference--> and we consider them to be most relevant for the studies and scenarios where our matching strategy might be applied.

This means that from the Unpaywall data we only consider records with publication year 2014 that have the `genre` `journal-article`. 

We obtained our set of Unpaywall articles similarly to how it is described in our [blog post on open access evidence in Unpaywall](https://subugoe.github.io/scholcomm_analytics/posts/unpaywall_evidence/). We will outline the main steps again in the following.

First, we downloaded the most recent Unpaywall data dump released in April 2019 from [here](https://s3-us-west-2.amazonaws.com/unpaywall-data-snapshots/) and imported it into a local MongoDB database. The snapshot contains more than 100 million records and hence, the file is about 100 GB large. To obtain a more manageable object to work with, we extracted the fields which are most relevant to our matching objective using the following query:

```{bash}
"C:\Program Files\MongoDB\Server\4.0\bin\mongo.exe"
db.unpaywallApr19.aggregate([
   {
      $match: {
          year: { $gte: 2014, $lte: 2016}
      }
   },
   {
      $project: {
          doi: 1,
          year: 1,
          genre: 1,
          title: 1,
          AuthorCount: { $cond: { if: { $isArray: "$z_authors" }, then: { $size: "$z_authors" }, else: null} },
          AuthorFirst: { $cond: { if: { $isArray: "$z_authors" }, then: { $arrayElemAt: [ "$z_authors", 0 ] }, else: null } },
          AuthorLast: { $cond: { if: { $isArray: "$z_authors" }, then: { $arrayElemAt: [ "$z_authors", -1 ] }, else: null } }
      }
   },
   {
      $out: 'authorInformation' 
   }
] )
```

Note that on a Linux or Mac OS, single quotes (`'`) and double quotes (`"`) need to be exchanged. We initially filtered for publication years between 2014 and 2016, extracted doi, publication year, article type (`genre`), article title, and some author information, namely the number of authors, and the given and family names of the first and last author. For single author articles, the first and last author coincide. We exported the resulting data as json file and loaded it as table into our [GoogleBigQuery](https://cloud.google.com/bigquery/) analytical environment (paid service, access protected), specifying a [schema](schema_bigquery.json). There, we added information on the journal (ISSN and journal title) to the data from a previous export based on the local MongoDB database. We joined the journal related fields on DOI, which was possible since every Unpaywall entry contains DOI information. We then exported the table as three different .csv files, one for each publication year and imported the one corresponding to the publication year 2014 into the oracle environment of the KB.

## Matching criteria

A number of features may be appropriate for matching articles between bibliographic databases <!-- reference?! -->. In the best case scenario, persistent identifiers such as DOIs can be used for direct matching. In the cases where persistent identifiers are missing, other characteristics of an article and its authorship may be used for matching, such as the article title, journal title or ISSNs. However, none of these features are 100 % unique (e.g. articles can share titles, or authors can share names) and thus using them in isolation may lead to unreliable matches. A combination of these characteristics can produce more precise matching.

In this report we focus on an initial subset of characteristics which may potentially be used for matching between WOS and Unpaywall, including publication years, article titles, ISSNs, and author counts. These criteria were selected primarily due to data availability and coverage in both databases, but are not exhaustive and may be supplemented with further characteristics in future (e.g. page numbers). 

A description of the selected matching criteria follows. Note that for the following sections, items in WoS are limited only to 'article' and 'review' document types, and Unpaywall items to 'journal-article' types.

### Publication years

An initial way to filter potentially matching records is via their registered publication years in the two datasets. However, the registered publication dates in WoS and Unpaywall sometimes differ (for example, because one takes the online publication date and the other the print publication date). Below we show the number of items that can be matched between our Unpaywall dataset (publication year = 2014) and all items in WoS, as a function of the WoS publication year:

```{r}

# DOI matches as a function of WOS publication year (Unpaywall publication year = 2014).

if(file.exists("data/criteria/publication_years_doi_matches.csv")){
  publication_years_doi_matches <- read_csv("data/criteria/publication_years_doi_matches.csv")
} else {
  publication_years_doi_matches <- dbGetQuery(con, read_file("sql/criteria/publication_years_doi_matches.sql"))
  write_csv(publication_years_doi_matches, "data/criteria/publication_years_doi_matches.csv")
}

publication_years_doi_matches %>%
  ggplot(aes(x=WOS_YEAR, y=MATCHES)) +
  geom_bar(stat="identity") +
  labs(x="WOS Publication Year", y="Number of DOI matched items", title = "How do publication years differ between Unpaywall (2014) and WOS\nfor items matched based on DOIs?") +
  theme_bw()

```

Most items have the same publication year, 2014, in WoS as in the Unpaywall dataset. However, a non-negligible proportion of items have publication years one or two years later in WoS than in Unpaywall. Based on this, we recommend to focus try to match items in Unpaywall with items from Wos having the same publication year, or one or two years later.

### Article titles

Article titles are potentially a good candidate for matching due to their high coverage: 

```{r}

# Coverage of titles, and proportion of unique titles in WoS and Unpaywall

article_title_wos_coverage <- dbGetQuery(con, read_file("sql/criteria/article_title_wos_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
article_title_upw_coverage <- dbGetQuery(con, read_file("sql/criteria/article_title_upw_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
article_title_wos_uniques <- dbGetQuery(con, read_file("sql/criteria/article_title_wos_uniques.sql")) %>% 
  pull(PCT_UNIQUE)
article_title_upw_uniques <- dbGetQuery(con, read_file("sql/criteria/article_title_upw_uniques.sql")) %>% 
  pull(PCT_UNIQUE)

```

`r wos_article_title_coverage` % of 'article' and 'review' type documents in WOS, and `r upw_article_title_coverage` % of 'journal-article' type documents in Unpaywall have a title. However, only `r wos_article_title_uniques` % of 'article' and 'review' titles in WOS are unique, compared to `r upw_article_title_uniques` of 'journal-articles' in Unpaywall, suggesting that direct matching of titles may lead to a high number of false-positive matches.

To better understand the the potential for using article titles for matching articles without DOIs, we can consider set of DOI-matched articles as a 'gold standard', and compare their title characteristics. We first check the percentage of DOI-matched articles that also have exactly matching titles:

```{r}

# Calculate percentage of doi-matched articles with exactly matching titles

article_title_doi_matches_exact <- dbGetQuery(con, read_file("sql/criteria/article_title_doi_matches_exact.sql")) %>%
  pull(PCT_EXACT_TITLE_MATCHES)

```

We find that only `r doi_matches_exact_titles` % of doi-matched articles have exactly matching titles. Reasons for this may be due to differences in character encoding, handling of special character, or sentence cases (i.e. upper vs lower-case) of titles processed by WoS and Unpaywall.

An alternative to exact matching is to consider approximate string matching, which would allow titles to be matched even if they possessed minor differences in a small number of characters. Oracle provides two built-in functions as part of the `UTL_MATCH` package for approximate string matching: `EDIT_DISTANCE_SIMILARITY()` AND `JARO_WINKLER_SIMILARITY()`. Both functions provide a similarity score that is normalised such that 1 equates to perfect similarity and 0 to no similarity (or 100 % and 0 % when given as percentages, respectively). `EDIT_DISTANCE_SIMILARITY()` provides a similarity score based upon the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) between two strings, which counts the number of edits (insertions, deletions, or substitutions) needed to convert one string to the other. `JARO_WINKLER_SIMILARITY()` provides a similarity score based on the [Jaro-Winkler distance](https://en.wikipedia.org/wiki/Jaro-Winkler_distance), which measures the number of characters in common and number of transpositions.

Below we plot a histogram of showing the proportion of matching titles in our DOI-matched set, as a function of the two similarity measures. To further improve the matching efficiency, we transform the title strings to lower case (using the Oracle `LOWER` function), and remove whitespace from the start and end of the string (using the Oracle `TRIM` function). Note that for performance reasons, we use a small sample of the Unpaywall results set (~1%, using the Oracle `SAMPLE(1)` function) for this initial testing phase:

```{r}

if(file.exists("data/criteria/article_title_doi_matches_similarity.csv")){
  article_title_doi_matches_similarity <- read_csv("data/criteria/article_title_doi_matches_similarity.csv")
} else {
  article_title_doi_matches_similarity <- dbGetQuery(con, read_file("sql/criteria/article_title_doi_matches_similarity.sql"))
  write_csv(article_title_doi_matches_similarity, "data/criteria/article_title_doi_matches_similarity.csv")
}

article_title_doi_matches_similarity %>%
  select(EDIT_DISTANCE_SIMILARITY, JARO_WINKLER_SIMILARITY) %>%
  gather("EDIT_DISTANCE_SIMILARITY", "JARO_WINKLER_SIMILARITY", 
         key="method", 
         value="similarity") %>%
  group_by(method, similarity) %>%
  summarise(n = n()) %>%
  mutate(freq = n/sum(n)) %>%
  ggplot(aes(x=similarity, y=freq, fill=method, color=method)) +
  geom_bar(stat="identity") +
  labs(y="Percentage of Articles Matched", x="Title Similarity") +
  theme_bw() +
  theme(legend.position = "none") +
  scale_y_continuous(breaks=seq(from=0, to=1, by=0.1), labels=scales::percent) +
  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +
  facet_wrap(~method)

```

These plots show that `EDIT_DISTANCE_SIMILARITY()` and `JARO_WINKLER_SIMILARITY()` have similar distributions of similarity scores for titles on DOI-matched articles. Both distributions show that titles are consistently matched with similarities > 80 % (with a small percentage of outliers), suggesting this as a suitable threshold below which matched titles should be considered unreliable.

An additional method to assess the efficacy of the different similarity matching methods is to assess matching similarity on randomly matched articles, i.e. assuming that random titles should not be the same:

```{r}

if(file.exists("data/criteria/article_title_random_matches_similarity.csv")){
  article_title_random_matches_similarity <- read_csv("data/criteria/article_title_random_matches_similarity.csv")
} else {
  article_title_random_matches_similarity <- dbGetQuery(con, read_file("sql/criteria/article_title_random_matches_similarity.sql"))
  write_csv(article_title_random_matches_similarity, "data/criteria/article_title_random_matches_similarity.csv")
}

article_title_random_matches_similarity %>%
 select(EDIT_DISTANCE_SIMILARITY, JARO_WINKLER_SIMILARITY) %>%
  gather("EDIT_DISTANCE_SIMILARITY", "JARO_WINKLER_SIMILARITY", 
         key="method", 
         value="similarity") %>%
  group_by(method, similarity) %>%
  summarise(n = n()) %>%
  mutate(freq = n/sum(n)) %>%
  ggplot(aes(x=similarity, y=freq, fill=method, color=method)) +
  geom_bar(stat="identity") +
  labs(y="Percentage Matched", x="Title Similarity") +
  theme_bw() +
  theme(legend.position = "none") +
  scale_y_continuous(breaks=seq(from=0, to=1, by=0.1), labels=scales::percent) +
  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +
  coord_cartesian(xlim=c(0,100)) +
  facet_wrap(~method)

```

These plots show that the `EDIT_DISTANCE_SIMILARITY()` and `JARO_WINKLER_SIMILARITY()` produce much lower similarity scores when used to compare random titles, than when comparing titles of doi-matched articles. These results suggest that matching via string similarity functions can be an effective method to increase the number of matches whilst retaining high precision. Notably, we can see different distribution patterns between the two similarity measures - it appears that `EDIT_DISTANCE_SIMILARITY()` produces much lower similarity scores (median = `r median(random_matches_title_similarity$EDIT_DISTANCE_SIMILARITY`) with less overlap with the distribution of DOI-matched articles, compared to `JARO_WINKLER_SIMILARITY()` (median = `r median(random_matches_title_similarity$JARO_WINKLER_SIMILARITY`).

We can thus interpret that `EDIT_DISTANCE_SIMILARITY()` is a more appropriate measure for matching titles. For matching between datasets we thus recommend to implement the `EDIT_DISTANCE_SIMILARITY()` function, with a similarity threshold of 80 %, for matching of article titles.

### Article title length

In the above section we match titles using Oracle built-in functions for title similarity. However, such string similarity algorithms are notoriously slow on large datasets, as they require a matrix of rows to be checked against every other row in the dataset (e.g. if we were to compare 10,000 Unpaywall records with 10,000 WoS records, we would need to perform 100,000,000 string similarity comparisons). A target for scaling such similarity functions is to reduce the number of pairwise comparisons between titles. One potential way to do this is by first considering differences in the title length, where titles with greatly differing lengths are unlikely to belong to the same record.

First we compare the difference in title lengths of Unpaywall and WoS items in our DOI-matched dataset:

```{r}

if(file.exists("data/criteria/article_title_doi_matches_length.csv")){
  article_title_doi_matches_length <- read_csv("data/criteria/article_title_doi_matches_length.csv")
} else {
  article_title_doi_matches_length <- dbGetQuery(con, read_file("sql/criteria/article_title_doi_matches_length.sql"))
  write_csv(article_title_doi_matches_length, "data/criteria/article_title_doi_matches_length.csv")
}

article_title_doi_matches_length %>%
  ggplot(aes(x=LENGTH_DIFFERENCE, y=N)) +
  geom_bar(stat="identity") +
  coord_cartesian(xlim=c(0,100)) +
  labs(x="Difference in Title Length", y="Number of DOI Matches") +
  theme_bw()

```

From this, we can see that the vast majority of titles do not differ significantly in length: `r 100*(article_title_doi_matches_length %>% filter(LENGTH_DIFFERENCE <= 10) %>% tally(N) %>% pull(n)) /(article_title_doi_matches_length %>% tally(N) %>% pull(n))` % of titles differ by less than 10 characters in length.

We can also compare the difference in title lengths for randomly matched article titles:

```{r}

if(file.exists("data/criteria/article_title_random_matches_length.csv")){
  article_title_random_matches_length <- read_csv("data/criteria/article_title_random_matches_length.csv")
} else {
  article_title_random_matches_length <- dbGetQuery(con, read_file("sql/criteria/article_title_random_matches_length.sql"))
  write_csv(article_title_random_matches_length, "data/criteria/article_title_random_matches_length.csv")
}

article_title_random_matches_length %>%
  ggplot(aes(x=LENGTH_DIFFERENCE, y=N)) +
  geom_bar(stat="identity") +
  coord_cartesian(xlim=c(0,100)) +
  labs(x="Difference in Title Length", y="Number of DOI Matches") +
  theme_bw()

```

The results show that there exists a wide variability in the length of titles when randomly matched, thus, by only comparing titles with similar lengths, we could exclude a large number of pairwise similarity comparisons between article titles. For example, excluding articles with differences in title lengths over 10 characters, would exclude `r 100*(article_title_random_matches_length %>% filter(LENGTH_DIFFERENCE >= 10) %>% tally(N) %>% pull(n)) /(article_title_random_matches_length %>% tally(N) %>% pull(n))` % of the pairwise comparisons required, greatly improving performance.

### Article title blacklist/duplicates/something

A secondary factor related to titles is that of potential duplicates, i.e. two distinct records that share the same title. In some cases, title names can be shared by a large number of items (e.g. "Letter to the Editor"):

```{r}

# Unpaywall duplicate titles
if(file.exists("data/criteria/article_title_upw_duplicates.csv")){
  article_title_upw_duplicates <- read_csv("data/criteria/article_title_upw_duplicates.csv")
} else {
  article_title_upw_duplicates <- dbGetQuery(con, read_file("sql/criteria/article_title_upw_duplicates.sql"))
  write_csv(article_title_upw_duplicates, "data/criteria/article_title_upw_duplicates.csv")
}

# WoS duplicate titles
if(file.exists("data/criteria/article_title_wos_duplicates.csv")){
  article_title_wos_duplicates <- read_csv("data/criteria/article_title_wos_duplicates.csv")
} else {
  article_title_wos_duplicates <- dbGetQuery(con, read_file("sql/criteria/article_title_wos_duplicates.sql"))
  write_csv(article_title_wos_duplicates, "data/criteria/article_title_wos_duplicates.csv")
}

article_title_upw_duplicates %>%
  arrange(desc(N)) %>%
  top_n(10) %>%
  ggplot(aes(x=reorder(ARTICLE_TITLE, N), y=N)) +
  geom_bar(stat="identity") +
  coord_flip() +
  theme_bw() +
  labs(x="Number of articles", y="", title="Number of duplicate articles for common titles in Unpaywall (top 10)")

article_title_wos_duplicates %>%
  arrange(desc(N)) %>%
  top_n(10) %>%
  ggplot(aes(x=reorder(ARTICLE_TITLE, N), y=N)) +
  geom_bar(stat="identity") +
  coord_flip() +
  theme_bw() +
  labs(x="Number of articles", y="", title="Number of duplicate articles for common titles in WoS (top 10)")

```

Due to these duplicate titles which cannot be definitively matched via title matching methods, our recommendation is to remove all items with duplicate titles from our datasets, and only match where unique titles exists.

During these title matching processes, we also noticed a number of examples where items might match due to high similarity of titles, when they represent related (but not the same) documents. For example, an article may have a corrigendum associated with it, with the title "Corrigendum: [original article title]". Clearly, these two records do not represent the same document, but may match due to the similarity between titles, and matching of other associated metadata (e.g. the corrigendum may be in the same journal and have the same authors as the original document).

<!-- Need to add this - how did we generate manual list? -->

### Journal Titles 

Journal titles present another possible facet for matching. First we check the coverage of journal titles in our Unpaywall and WoS data:

```{r}

journal_title_wos_coverage <- dbGetQuery(con, read_file("sql/criteria/journal_title_wos_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
journal_title_upw_coverage <- dbGetQuery(con, read_file("sql/criteria/journal_title_upw_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

```

Our results show a high level of coverage, with `r journal_title_wos_coverage` % of items in WoS, and `r journal_title_upw_coverage` % of items in Unpaywall being associated with a journal title.

We then check the percentage of articles that can be matched via DOIs, that also possess the exact same journal title. We apply the Oracle functions `LOWER` and `TRIM` to the journal titles before matching, to improve efficiency:

```{r}
journal_title_doi_matches_exact <- dbGetQuery(con, read_file("sql/criteria/journal_title_doi_matches_exact.sql")) %>%
  pull(PCT_EXACT_JOURNAL_MATCHES)
```

Despite a high level of coverage of titles in both databases, only `r journal_title_doi_matches_exact` % of journal titles match exactly, for articles that can be matched together via DOIs. The reason for this is either minor changes in journal titles between datasets due to character encoding issues, or inconsistent reporting of journal titles between databases (e.g "PNAS" versus "Proceedings of the National Academy of Sciences").

### ISSNs

Given the weakness of matching with journal titles alone, a potential altnernative is to match on the journal ISSN. As before, we first check the coverage of ISSNs within each dataset:

```{r}

issn_wos_coverage <- dbGetQuery(con, read_file("sql/criteria/issn_wos_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
issn_upw_coverage <- dbGetQuery(con, read_file("sql/criteria/issn_upw_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

```

Coverage of ISSNs in both databases is also high, with `r issn_wos_coverage` % of items in WoS, and `r issn_upw_coverage` $ of items in Unpaywall possessing ISSN information.

We compare the percentage of DOI-matched items that have a matching ISSN. Note that some records in Unpaywall have multiple ISSNs associated with them <!-- Is it just for Unpaywall?? -->, whilst WoS records are only associated with a single ISSN. We thus match dependent on whether ANY of the ISSNs associated with an item in Unpaywall are the same as the ISSN for an item in WoS. Note that ISSNs in Unpaywall are parsed to a separate table (documented in the following section on data preprocessing):

```{r}

issn_doi_matches_exact <- dbGetQuery(con, read_file("sql/criteria/issn_doi_matches_exact.sql")) %>%
  pull(PCT_EXACT_ISSN_MATCHES)

```

We find that `r issn_doi_matches_exact` % of DOI-matched articles also have a matching ISSN, a higher rate than for journal titles alone. 

### Author counts

The final feature that we investigate here as a potential matching option is for author counts, i.e. matched articles should have the same number of authors. Author count data is calculated within the KB database directly <!-- Should we check how this is calculated??? -->, whilst author counts for Unpaywall are calculate from the length of the "z_authors" field (see above section on "Data selection and acquistion"). First we check the coverage of author counts in both datasets:

```{r}
authorcount_wos_coverage <- dbGetQuery(con, read_file("sql/criteria/authorcount_wos_coverage.sql")) %>% 
  pull(PCT_COVERAGE)
authorcount_upw_coverage <- dbGetQuery(con, read_file("sql/criteria/authorcount_upw_coverage.sql")) %>% 
  pull(PCT_COVERAGE)

```

We then check the number of DOI-matched articles that also have exactly matching number of authors:

```{r}

authorcount_doi_matches_exact <- dbGetQuery(con, read_file("sql/criteria/authorcount_doi_matches_exact.sql")) %>%
  pull(PCT_EXACT_AUTHORCOUNT_MATCHES)

```

`r issn_doi_matches_exact` % of articles that can be matched by DOIs also have matching numbers of authors. This high percentage means that we recommend to also use author counts as a matching feature.

## Data preprocessing

For matching of records between WoS and Unpaywall, a necessary first step is to clean and normalise the data, both to improve matching efficiency and performance of SQL queries. For example, when matching on the DOI field, it is necessary that both DOIs are in the same letter case, and do not contain any superfluous characters (e.g. leading and trailing whitespaces), otherwise matches may be missed.

For both Unpaywall and WoS we applied the following cleaning and normalisation procedures:

DOIs: Converted to lowercase (Oracle `LOWER()` function) and trimmed leading and trailing whitespace (Oracle `TRIM()` function)
Article titles: Converted to lowercase and trimmed leading and trailing whitespace
Journal titles: Converted to lowercase and trimmed leading and trailing whitespace

<!-- not needed anymore?

Author names: Author names were 'blocked' by concatenating the first initial of the first name, and the full last name (following the approach of e.g. Caron and Van Eck, 2014 #Ref list? , for large-scale author disambiguation). Author names were first converted to lowercase and leading and trailing whitespace was trimmed. A REGEX function was used to remove special characters from last names prior to concatenation (in Oracle: `REGEXP_REPLACE(NAME, '[^A-Za-z]','')`).-->

As mentioned before, we restricted the analysis to document types `journal-article` (Unpaywall) or `article` and `review` (WoS). From the imported Unpaywall table, and the WoS-KB database, we retrieved the fields we found to be potentially usefull in the previous section, namely:

- DOI
- Publication year
- Article title
- Length of the article title
- Author count
- Journal name
- Journal ISSNs

As shown in the previous section on potential matching criteria, both datasets contain articles titles which occur in multiple rows. We decided to exclude all of these titles from our matching candidates since we want to think of an article's title as unique for this publication. We also exluded all titles starting with one of the keywords contained in [this list](title_keyword_blacklist.csv), e.g. "correction", "erratum", or "addendum" since they would otherwise wrongly be matched with the original articles.

In order to speed up matching we created an index for the DOI column in both tables.


```{r}
#Prepare Unpaywall tables for matching, only run once

# dbGetQuery(con, read_file("sql/preprocessing/create_upw_14_norm.sql"))
# dbGetQuery(con, read_file("sql/preprocessing/create_upw_14_norm_index.sql"))
# dbGetQuery(con, read_file("sql/preprocessing/create_upw_14_issns.sql"))

#Prepare WoS tables for matching, only run once

# dbGetQuery(con, read_file("sql/preprocessing/create_wos_14_16_norm.sql"))
# dbGetQuery(con, read_file("sql/preprocessing/create_wos_14_16_norm_index.sql"))

```


## Matching algorithm

<!--(Anne/Nick)
Description of algorithm-->

The first step of our matching routine is simply using DOIs where they exist and are present in both databases. The more interesting part is the matching of articles without DOI information based on the criteria discussed in the previous section. As mentioned before, we only consider `journal article`s from Unpaywall and `articles`s and `review`s from WoS. We only try to match articles where the publication year from WoS is the same as or up to two years after the one in Unpaywall. Since all Unpaywall articles we investigated in this analysis are from 2014, we perform the matching on WoS articles with publication year between 2014 and 2016.



Since the comparison of titles to determine their similarity measure is costly, we try to narrow down the subset of possible matching candidates as far as possible using the following criteria:

- pubyear: As mentioned, before we require the publication year in WoS to be the same as or up to two years after the Unpaywall publication year. This is already implemented in the preprocessing of the two matching tables.
- journal information: We restrict matching candidates to records with the same ISSN or (exactly) the same journal title. When multiple ISSNs are associated with an article (e.g. of the print and electronic version of the journal), we compare to all of them.
- author count: We compare only articles with coinciding number of authors.
- length of article title: We compare only articles where the lengths of the article titles differ no more than 10 characters.

Then, we classify as matched articles the ones which have a title similarity (edit distance) of more than 80 percent.

This results in the query stored as [create_upw14_wos_matching_results.sql](create_upw14_wos_matching_results.sql) to obtain a list of matchings.

To evaluate the performance of our matching algorithm, we also run the matching algorithm on the two matching tables without using any DOI information (see the query [create_upw_14_wos_matching_results_eval.sql](create_upw_14_wos_matching_results_eval.sql)).

## Results

## Discussion and Recommendations

### Runtime

Preprocessing the data to generate the normalized and cleaned up tables `upw_14_norm` and `wos_14_16_norm` takes several minutes each.

# Bibliography

